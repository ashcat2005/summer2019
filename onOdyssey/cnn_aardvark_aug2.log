Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-08-03 10:53:40.785814: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-03 10:53:40.804218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095255000 Hz
2019-08-03 10:53:40.804439: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55acad803320 executing computations on platform Host. Devices:
2019-08-03 10:53:40.804477: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
0
286
95
augmenting
95
returning
<class 'list'>
200
augmenting
95
returning
augmenting
96
returning
1
16
5
augmenting
5
returning
<class 'list'>
200
augmenting
5
returning
augmenting
6
returning
2
76
25
augmenting
25
returning
<class 'list'>
200
augmenting
25
returning
augmenting
26
returning
3
18
6
augmenting
6
returning
<class 'list'>
200
augmenting
6
returning
augmenting
6
returning
4
9
3
augmenting
3
returning
<class 'list'>
200
augmenting
3
returning
augmenting
3
returning
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
Train on 1000 samples, validate on 1000 samples
Epoch 1/25

1000/1000 [==============================] - 198s 198ms/step - loss: 1.6094 - acc: 0.1910 - val_loss: 1.5823 - val_acc: 0.2240
Epoch 2/25

1000/1000 [==============================] - 185s 185ms/step - loss: 1.4993 - acc: 0.2930 - val_loss: 1.5351 - val_acc: 0.2510
Epoch 3/25

1000/1000 [==============================] - 186s 186ms/step - loss: 1.3293 - acc: 0.8140 - val_loss: 1.6658 - val_acc: 0.2470
Epoch 4/25

1000/1000 [==============================] - 187s 187ms/step - loss: 1.2202 - acc: 0.4310 - val_loss: 1.6312 - val_acc: 0.3950
Epoch 5/25

1000/1000 [==============================] - 187s 187ms/step - loss: 1.2231 - acc: 0.5580 - val_loss: 1.6118 - val_acc: 0.3810
Epoch 6/25

1000/1000 [==============================] - 184s 184ms/step - loss: 1.0162 - acc: 0.6770 - val_loss: 1.7748 - val_acc: 0.3230
Epoch 7/25

1000/1000 [==============================] - 185s 185ms/step - loss: 0.9023 - acc: 0.7970 - val_loss: 1.9040 - val_acc: 0.2630
Epoch 8/25

1000/1000 [==============================] - 185s 185ms/step - loss: 0.8107 - acc: 0.8500 - val_loss: 1.8742 - val_acc: 0.4530
Epoch 9/25

1000/1000 [==============================] - 186s 186ms/step - loss: 0.6967 - acc: 0.8310 - val_loss: 1.9445 - val_acc: 0.4190
Epoch 10/25

1000/1000 [==============================] - 187s 187ms/step - loss: 0.6283 - acc: 0.8110 - val_loss: 2.1194 - val_acc: 0.3900
Epoch 11/25

1000/1000 [==============================] - 187s 187ms/step - loss: 0.5417 - acc: 0.8800 - val_loss: 2.3324 - val_acc: 0.3420
Epoch 12/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.4831 - acc: 0.9200 - val_loss: 2.3386 - val_acc: 0.4410
Epoch 13/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.4325 - acc: 0.8570 - val_loss: 2.4551 - val_acc: 0.3440
Epoch 14/25

1000/1000 [==============================] - 188s 188ms/step - loss: 0.3764 - acc: 0.9270 - val_loss: 2.5973 - val_acc: 0.3360
Epoch 15/25

1000/1000 [==============================] - 190s 190ms/step - loss: 0.3253 - acc: 0.9450 - val_loss: 2.6746 - val_acc: 0.4570
Epoch 16/25

1000/1000 [==============================] - 188s 188ms/step - loss: 0.2957 - acc: 0.9220 - val_loss: 2.8378 - val_acc: 0.3310
Epoch 17/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.2518 - acc: 0.9590 - val_loss: 2.9751 - val_acc: 0.2830
Epoch 18/25

1000/1000 [==============================] - 188s 188ms/step - loss: 0.2265 - acc: 0.9640 - val_loss: 2.9613 - val_acc: 0.3720
Epoch 19/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.1987 - acc: 0.9550 - val_loss: 3.1670 - val_acc: 0.3470
Epoch 20/25

1000/1000 [==============================] - 190s 190ms/step - loss: 0.1697 - acc: 0.9710 - val_loss: 3.4609 - val_acc: 0.2690
Epoch 21/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.1573 - acc: 0.9750 - val_loss: 3.4426 - val_acc: 0.3120
Epoch 22/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.1328 - acc: 0.9820 - val_loss: 3.5214 - val_acc: 0.3190
Epoch 23/25

1000/1000 [==============================] - 191s 191ms/step - loss: 0.1208 - acc: 0.9730 - val_loss: 3.7609 - val_acc: 0.2940
Epoch 24/25

1000/1000 [==============================] - 189s 189ms/step - loss: 0.1051 - acc: 0.9870 - val_loss: 3.9750 - val_acc: 0.2830
Epoch 25/25

1000/1000 [==============================] - 190s 190ms/step - loss: 0.0951 - acc: 0.9860 - val_loss: 3.9806 - val_acc: 0.3250
[[1.1626027e-07 1.5076947e-21 9.9210197e-01 7.8978995e-03 9.2200403e-23]
 [3.1626890e-03 9.2992163e-01 6.6914462e-02 1.0935950e-06 1.5434749e-07]
 [1.4815968e-01 2.8425477e-02 5.0660461e-01 2.3316188e-01 8.3648354e-02]
 ...
 [4.1723296e-01 6.6383081e-03 6.6072956e-02 5.8052551e-02 4.5200330e-01]
 [3.7683976e-01 4.0116861e-02 3.0867013e-01 6.7980878e-02 2.0639236e-01]
 [1.7266193e-02 2.8299828e-04 9.8245078e-01 1.0110242e-27 3.2601605e-31]]
[2 1 2 2 0 4 2 2 3 0 1 1 0 2 0 3 3 0 0 0 4 3 2 2 2 0 2 3 3 0 3 1 2 4 2 0 0
 2 3 0 2 3 2 1 2 1 0 3 4 4 2 2 0 3 2 2 2 3 2 2 0 0 2 0 3 1 2 0 2 3 0 0 1 0
 2 2 2 0 4 2 0 2 2 2 2 0 0 2 1 2 3 2 2 0 0 2 1 1 2 3 2 2 1 1 4 2 0 2 0 2 2
 0 2 3 1 2 0 2 1 2 0 2 0 2 1 0 2 0 0 2 0 3 2 2 3 0 0 2 2 2 2 2 0 2 0 0 0 0
 4 2 2 3 0 1 2 2 1 2 4 0 2 2 2 0 2 3 0 4 3 3 2 0 2 2 3 2 0 3 0 1 1 0 0 1 2
 0 0 1 0 1 3 2 0 2 2 2 2 0 3 1 0 0 2 2 0 0 0 0 2 0 1 2 0 2 4 2 0 3 0 3 4 2
 2 2 2 3 0 0 2 0 3 0 2 1 2 1 1 0 0 2 0 0 2 0 2 2 0 1 0 2 1 1 0 1 1 2 2 0 0
 1 0 0 3 2 0 0 2 4 2 2 2 0 0 2 3 2 2 0 0 0 0 1 3 0 1 0 2 2 0 2 3 2 3 2 2 4
 3 3 2 0 2 0 0 2 0 4 2 4 2 2 3 2 2 1 0 3 2 1 0 2 2 2 1 2 2 3 2 0 0 0 2 0 3
 2 2 1 0 0 2 0 0 3 2 3 2 2 0 0 0 3 2 1 2 2 0 2 0 0 1 0 0 0 0 0 2 2 2 4 2 2
 4 1 0 2 0 2 2 3 2 2 0 2 2 2 3 0 0 0 0 0 2 2 2 2 0 2 2 0 3 2 2 2 2 0 4 0 0
 4 3 4 0 2 2 0 2 3 4 3 0 1 0 2 3 1 2 1 0 2 2 0 2 2 2 2 1 2 0 0 0 2 2 3 4 2
 2 0 2 0 2 2 2 2 0 2 2 2 2 0 4 2 0 2 2 0 1 2 4 2 0 0 2 0 0 2 2 3 0 2 0 0 0
 2 2 0 1 0 2 2 2 3 3 0 0 2 3 2 0 4 4 2 3 2 1 2 1 0 0 4 2 2 2 0 0 0 3 2 2 0
 2 0 0 0 2 2 2 2 2 1 0 2 0 2 2 0 2 3 1 2 2 2 0 0 0 2 3 0 3 0 2 3 2 1 0 2 2
 1 0 0 0 2 0 2 0 2 1 4 0 0 2 1 0 0 2 2 2 0 3 0 4 1 3 2 3 2 2 3 2 2 3 2 3 0
 4 3 3 2 2 0 2 2 2 1 2 0 4 2 0 2 2 2 1 2 2 2 2 0 0 0 2 3 0 3 2 0 0 2 4 0 0
 0 3 2 2 3 4 2 2 0 3 0 4 1 4 2 2 0 0 2 2 2 0 0 2 0 2 2 2 2 0 2 3 2 0 0 2 2
 3 2 2 0 3 2 0 2 2 2 1 3 0 2 2 0 4 2 2 2 3 0 0 3 0 2 0 2 2 0 1 0 4 2 2 2 2
 2 0 2 2 2 1 0 0 2 0 2 2 2 0 0 2 0 2 2 2 2 2 2 3 2 0 0 0 2 0 3 2 0 0 0 2 2
 1 4 0 0 4 0 2 2 2 0 2 2 0 0 3 0 2 2 0 2 3 2 2 0 0 1 2 0 0 0 2 2 1 0 3 2 2
 0 0 2 2 2 2 0 0 0 3 0 0 0 3 0 2 3 2 0 2 2 2 2 2 2 3 0 2 0 2 0 1 0 0 0 0 2
 1 3 3 1 4 2 0 3 2 1 0 2 0 3 2 2 2 0 2 2 2 0 2 3 1 0 1 2 0 1 2 0 2 2 0 2 2
 0 4 1 0 2 2 2 0 2 0 2 0 1 3 2 1 2 2 3 1 0 1 0 2 0 3 1 0 0 1 0 3 2 0 3 3 2
 2 2 3 2 0 0 2 0 1 0 0 2 0 3 0 2 3 2 2 1 0 0 0 2 1 0 0 2 2 1 0 0 0 0 0 4 1
 2 4 2 2 0 2 0 2 0 0 2 2 3 0 2 2 2 2 1 2 0 2 0 2 3 0 2 0 0 2 2 2 0 3 2 2 2
 0 0 0 4 0 2 0 2 2 0 4 2 0 2 0 0 0 3 3 0 0 2 0 2 2 0 3 0 2 2 2 2 2 0 1 4 0
 2]

  32/1000 [..............................] - ETA: 54s
  64/1000 [>.............................] - ETA: 52s
  96/1000 [=>............................] - ETA: 51s
 128/1000 [==>...........................] - ETA: 49s
 160/1000 [===>..........................] - ETA: 47s
 192/1000 [====>.........................] - ETA: 45s
 224/1000 [=====>........................] - ETA: 43s
 256/1000 [======>.......................] - ETA: 41s
 288/1000 [=======>......................] - ETA: 39s
 320/1000 [========>.....................] - ETA: 37s
 352/1000 [=========>....................] - ETA: 35s
 384/1000 [==========>...................] - ETA: 34s
 416/1000 [===========>..................] - ETA: 32s
 448/1000 [============>.................] - ETA: 30s
 480/1000 [=============>................] - ETA: 28s
 512/1000 [==============>...............] - ETA: 26s
 544/1000 [===============>..............] - ETA: 24s
 576/1000 [================>.............] - ETA: 23s
 608/1000 [=================>............] - ETA: 21s
 640/1000 [==================>...........] - ETA: 19s
 672/1000 [===================>..........] - ETA: 17s
 704/1000 [====================>.........] - ETA: 16s
 736/1000 [=====================>........] - ETA: 14s
 768/1000 [======================>.......] - ETA: 12s
 800/1000 [=======================>......] - ETA: 10s
 832/1000 [=======================>......] - ETA: 9s 
 864/1000 [========================>.....] - ETA: 7s
 896/1000 [=========================>....] - ETA: 5s
 928/1000 [==========================>...] - ETA: 3s
 960/1000 [===========================>..] - ETA: 2s
 992/1000 [============================>.] - ETA: 0s
1000/1000 [==============================] - 53s 53ms/step
Test loss: 3.192302875518799
Test accuracy: 0.246
Traceback (most recent call last):
  File "cnn_aardvark_aug.py", line 166, in <module>
    main()
  File "cnn_aardvark_aug.py", line 162, in main
    cm = confusion_matrix(y_test, y_pred)
  File "/n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/sklearn/metrics/classification.py", line 253, in confusion_matrix
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
  File "/n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/sklearn/metrics/classification.py", line 81, in _check_targets
    "and {1} targets".format(type_true, type_pred))
ValueError: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets

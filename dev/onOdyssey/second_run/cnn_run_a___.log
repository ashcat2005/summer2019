Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:18.786944: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:18.790963: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:18.791084: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55dae6828fb0 executing computations on platform Host. Devices:
2019-11-07 09:04:18.791104: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/35

348/348 [==============================] - 26s 75ms/step - loss: 0.6929 - acc: 0.4971 - val_loss: 0.6706 - val_acc: 0.5948
Epoch 2/35

348/348 [==============================] - 20s 59ms/step - loss: 0.6552 - acc: 0.8046 - val_loss: 0.6570 - val_acc: 0.5690
Epoch 3/35

348/348 [==============================] - 20s 58ms/step - loss: 0.6122 - acc: 0.6983 - val_loss: 0.6467 - val_acc: 0.5776
Epoch 4/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5694 - acc: 0.7902 - val_loss: 0.6568 - val_acc: 0.5690
Epoch 5/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5298 - acc: 0.7414 - val_loss: 0.6678 - val_acc: 0.6034
Epoch 6/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5022 - acc: 0.7500 - val_loss: 0.7451 - val_acc: 0.5172
Epoch 7/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5004 - acc: 0.6868 - val_loss: 0.6973 - val_acc: 0.6034
Epoch 8/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4494 - acc: 0.8017 - val_loss: 0.7196 - val_acc: 0.6207
Epoch 9/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4428 - acc: 0.7730 - val_loss: 0.7305 - val_acc: 0.6724
Epoch 10/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4087 - acc: 0.8563 - val_loss: 0.7727 - val_acc: 0.5517
Epoch 11/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4002 - acc: 0.7902 - val_loss: 0.8004 - val_acc: 0.5690
Epoch 12/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3888 - acc: 0.7874 - val_loss: 0.8072 - val_acc: 0.6293
Epoch 13/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3692 - acc: 0.8707 - val_loss: 0.8238 - val_acc: 0.6638
Epoch 14/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3563 - acc: 0.8764 - val_loss: 0.8545 - val_acc: 0.6466
Epoch 15/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3455 - acc: 0.8678 - val_loss: 0.8941 - val_acc: 0.6552
Epoch 16/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3295 - acc: 0.8879 - val_loss: 0.9476 - val_acc: 0.6638
Epoch 17/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3143 - acc: 0.8966 - val_loss: 1.0056 - val_acc: 0.5948
Epoch 18/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3073 - acc: 0.8678 - val_loss: 1.0255 - val_acc: 0.6638
Epoch 19/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2918 - acc: 0.9023 - val_loss: 1.0464 - val_acc: 0.6379
Epoch 20/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2836 - acc: 0.9052 - val_loss: 1.0797 - val_acc: 0.6552
Epoch 21/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2730 - acc: 0.9052 - val_loss: 1.1288 - val_acc: 0.6552
Epoch 22/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2622 - acc: 0.9109 - val_loss: 1.1670 - val_acc: 0.6552
Epoch 23/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2542 - acc: 0.9167 - val_loss: 1.1822 - val_acc: 0.6552
Epoch 24/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2440 - acc: 0.9109 - val_loss: 1.2110 - val_acc: 0.6466
Epoch 25/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2357 - acc: 0.9080 - val_loss: 1.2400 - val_acc: 0.6638
Epoch 26/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2276 - acc: 0.9397 - val_loss: 1.2461 - val_acc: 0.6897
Epoch 27/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2172 - acc: 0.9483 - val_loss: 1.2492 - val_acc: 0.6638
Epoch 28/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2121 - acc: 0.9195 - val_loss: 1.2663 - val_acc: 0.6724
Epoch 29/35

348/348 [==============================] - 20s 57ms/step - loss: 0.2006 - acc: 0.9483 - val_loss: 1.2876 - val_acc: 0.6552
Epoch 30/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1953 - acc: 0.9626 - val_loss: 1.2806 - val_acc: 0.6552
Epoch 31/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1877 - acc: 0.9368 - val_loss: 1.2904 - val_acc: 0.6724
Epoch 32/35

348/348 [==============================] - 20s 57ms/step - loss: 0.1782 - acc: 0.9598 - val_loss: 1.3063 - val_acc: 0.6638
Epoch 33/35

348/348 [==============================] - 20s 57ms/step - loss: 0.1740 - acc: 0.9655 - val_loss: 1.2939 - val_acc: 0.6724
Epoch 34/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1669 - acc: 0.9511 - val_loss: 1.3012 - val_acc: 0.6983
Epoch 35/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1578 - acc: 0.9684 - val_loss: 1.3135 - val_acc: 0.6552
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.7323191e-01 2.6768059e-02]
 [2.6598907e-01 7.3401093e-01]
 [9.9999976e-01 1.8623579e-07]
 [9.7082144e-01 2.9178506e-02]
 [9.8289365e-01 1.7106416e-02]
 [4.4077915e-01 5.5922085e-01]
 [4.8241013e-04 9.9951756e-01]
 [1.0000000e+00 1.1194969e-10]
 [9.8766834e-01 1.2331610e-02]
 [1.0000000e+00 2.3451202e-08]
 [9.7826320e-01 2.1736735e-02]
 [5.3721368e-01 4.6278626e-01]
 [3.5431543e-01 6.4568454e-01]
 [5.4819620e-01 4.5180377e-01]
 [3.0564979e-01 6.9435012e-01]
 [9.9999750e-01 2.4954927e-06]
 [2.8039798e-01 7.1960205e-01]
 [5.9003288e-01 4.0996718e-01]
 [9.9608648e-01 3.9134738e-03]
 [7.9825782e-04 9.9920171e-01]
 [4.8241013e-04 9.9951756e-01]
 [2.8039798e-01 7.1960205e-01]
 [1.6865114e-04 9.9983132e-01]
 [9.8289365e-01 1.7106416e-02]
 [5.4819620e-01 4.5180377e-01]
 [5.1132458e-01 4.8867545e-01]
 [5.5336988e-01 4.4663015e-01]
 [9.9819213e-01 1.8078345e-03]
 [6.0185450e-01 3.9814553e-01]
 [3.0564979e-01 6.9435012e-01]
 [6.0225028e-01 3.9774972e-01]
 [4.6886766e-01 5.3113234e-01]
 [3.3300164e-01 6.6699839e-01]
 [4.7185251e-01 5.2814746e-01]
 [1.0000000e+00 1.4620130e-11]
 [2.0441203e-03 9.9795592e-01]
 [1.0000000e+00 1.4197086e-09]
 [5.3721368e-01 4.6278626e-01]
 [4.2230174e-01 5.7769823e-01]
 [6.0225028e-01 3.9774972e-01]
 [8.4126651e-01 1.5873349e-01]
 [1.8191192e-01 8.1808805e-01]
 [7.0302258e-10 1.0000000e+00]
 [9.9999976e-01 1.8623579e-07]
 [3.0228403e-01 6.9771600e-01]
 [9.3455106e-01 6.5448932e-02]
 [9.4412011e-01 5.5879947e-02]
 [6.3294494e-01 3.6705500e-01]
 [2.0916195e-01 7.9083812e-01]
 [1.0000000e+00 1.4197086e-09]
 [2.6726020e-03 9.9732745e-01]
 [1.0000000e+00 1.4197086e-09]
 [9.9999750e-01 2.4954927e-06]
 [9.8203546e-01 1.7964477e-02]
 [6.4778459e-01 3.5221538e-01]
 [9.9301589e-01 6.9841472e-03]
 [4.6049520e-01 5.3950471e-01]
 [6.0225028e-01 3.9774972e-01]
 [4.6652818e-01 5.3347182e-01]
 [9.8289365e-01 1.7106416e-02]
 [1.0000000e+00 1.1194969e-10]
 [5.1132458e-01 4.8867545e-01]
 [6.0225028e-01 3.9774972e-01]
 [6.7867047e-01 3.2132953e-01]
 [4.8241013e-04 9.9951756e-01]
 [1.6067127e-02 9.8393285e-01]
 [6.7867047e-01 3.2132953e-01]
 [2.0916195e-01 7.9083812e-01]
 [1.8642107e-01 8.1357902e-01]
 [5.7804662e-01 4.2195341e-01]
 [9.2713140e-07 9.9999905e-01]
 [3.6965045e-01 6.3034952e-01]
 [2.0916195e-01 7.9083812e-01]
 [2.3214351e-12 1.0000000e+00]
 [1.8191192e-01 8.1808805e-01]
 [5.3027922e-01 4.6972081e-01]
 [5.3897417e-01 4.6102586e-01]
 [1.0000000e+00 1.1194969e-10]
 [5.3721368e-01 4.6278626e-01]
 [5.3721368e-01 4.6278626e-01]
 [9.9999750e-01 2.4954927e-06]
 [8.2428628e-01 1.7571370e-01]
 [4.2822814e-01 5.7177186e-01]
 [6.3294494e-01 3.6705500e-01]
 [2.6760992e-01 7.3239011e-01]
 [1.0000000e+00 2.3451202e-08]
 [1.0000000e+00 1.4967394e-09]
 [4.6886766e-01 5.3113234e-01]
 [9.9999940e-01 6.3897915e-07]
 [3.4309819e-01 6.5690178e-01]
 [3.3300164e-01 6.6699839e-01]
 [3.7250420e-01 6.2749577e-01]
 [9.7236550e-01 2.7634533e-02]
 [9.9999356e-01 6.3974890e-06]
 [9.9862266e-01 1.3773902e-03]
 [9.7082144e-01 2.9178506e-02]
 [1.0000000e+00 1.1194969e-10]
 [4.8071295e-01 5.1928699e-01]
 [3.3300164e-01 6.6699839e-01]
 [5.0742269e-01 4.9257737e-01]
 [8.1147963e-01 1.8852039e-01]
 [7.0302258e-10 1.0000000e+00]
 [9.9999976e-01 1.8623579e-07]
 [6.2053645e-01 3.7946355e-01]
 [6.0425395e-01 3.9574605e-01]
 [1.0000000e+00 2.3451202e-08]
 [1.2836515e-02 9.8716342e-01]
 [5.4016685e-01 4.5983306e-01]
 [1.0000000e+00 1.8302159e-17]
 [6.4778459e-01 3.5221538e-01]
 [6.0225028e-01 3.9774972e-01]
 [8.5734463e-01 1.4265542e-01]
 [6.0225028e-01 3.9774972e-01]
 [1.0000000e+00 1.8302159e-17]
 [3.1101251e-01 6.8898749e-01]
 [4.2632836e-01 5.7367164e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0
 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1
 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0
 0 0 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 19ms/step
Test loss: 2.082630424663938
Test accuracy: 0.6896551683031279
[[47 11]
 [25 33]]

Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:47.105603: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:47.163899: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:47.164148: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559ab8c55170 executing computations on platform Host. Devices:
2019-11-07 09:04:47.164201: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/35

348/348 [==============================] - 24s 70ms/step - loss: 0.6933 - acc: 0.4483 - val_loss: 0.6842 - val_acc: 0.5431
Epoch 2/35

348/348 [==============================] - 22s 62ms/step - loss: 0.6751 - acc: 0.6868 - val_loss: 0.6750 - val_acc: 0.5517
Epoch 3/35

348/348 [==============================] - 21s 61ms/step - loss: 0.6530 - acc: 0.6868 - val_loss: 0.6655 - val_acc: 0.5862
Epoch 4/35

348/348 [==============================] - 21s 61ms/step - loss: 0.6261 - acc: 0.6954 - val_loss: 0.6866 - val_acc: 0.4914
Epoch 5/35

348/348 [==============================] - 21s 61ms/step - loss: 0.6140 - acc: 0.5833 - val_loss: 0.7632 - val_acc: 0.5603
Epoch 6/35

348/348 [==============================] - 21s 61ms/step - loss: 0.6975 - acc: 0.5546 - val_loss: 0.6853 - val_acc: 0.5172
Epoch 7/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5799 - acc: 0.6466 - val_loss: 0.7750 - val_acc: 0.5086
Epoch 8/35

348/348 [==============================] - 21s 61ms/step - loss: 0.6385 - acc: 0.5603 - val_loss: 0.6802 - val_acc: 0.6034
Epoch 9/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5579 - acc: 0.6552 - val_loss: 0.7272 - val_acc: 0.5862
Epoch 10/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5965 - acc: 0.6264 - val_loss: 0.7076 - val_acc: 0.5517
Epoch 11/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5319 - acc: 0.6868 - val_loss: 0.7716 - val_acc: 0.5086
Epoch 12/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5651 - acc: 0.6379 - val_loss: 0.7121 - val_acc: 0.5948
Epoch 13/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5094 - acc: 0.7270 - val_loss: 0.7513 - val_acc: 0.5948
Epoch 14/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5354 - acc: 0.6580 - val_loss: 0.7461 - val_acc: 0.5086
Epoch 15/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4884 - acc: 0.7328 - val_loss: 0.7992 - val_acc: 0.5259
Epoch 16/35

348/348 [==============================] - 21s 61ms/step - loss: 0.5091 - acc: 0.6839 - val_loss: 0.7643 - val_acc: 0.5776
Epoch 17/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4715 - acc: 0.7557 - val_loss: 0.7951 - val_acc: 0.5948
Epoch 18/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4830 - acc: 0.7356 - val_loss: 0.8090 - val_acc: 0.5259
Epoch 19/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4566 - acc: 0.7500 - val_loss: 0.8384 - val_acc: 0.5345
Epoch 20/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4583 - acc: 0.7155 - val_loss: 0.8352 - val_acc: 0.5862
Epoch 21/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4457 - acc: 0.7586 - val_loss: 0.8504 - val_acc: 0.5948
Epoch 22/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4345 - acc: 0.7931 - val_loss: 0.8913 - val_acc: 0.5259
Epoch 23/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4359 - acc: 0.7414 - val_loss: 0.8888 - val_acc: 0.5776
Epoch 24/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4137 - acc: 0.7644 - val_loss: 0.9217 - val_acc: 0.5948
Epoch 25/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4244 - acc: 0.7816 - val_loss: 0.9299 - val_acc: 0.6121
Epoch 26/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3996 - acc: 0.7989 - val_loss: 0.9716 - val_acc: 0.5776
Epoch 27/35

348/348 [==============================] - 21s 61ms/step - loss: 0.4064 - acc: 0.7701 - val_loss: 0.9807 - val_acc: 0.5690
Epoch 28/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3937 - acc: 0.8017 - val_loss: 1.0019 - val_acc: 0.5690
Epoch 29/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3834 - acc: 0.7989 - val_loss: 1.0414 - val_acc: 0.5862
Epoch 30/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3876 - acc: 0.7787 - val_loss: 1.0469 - val_acc: 0.6379
Epoch 31/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3690 - acc: 0.8046 - val_loss: 1.0722 - val_acc: 0.5690
Epoch 32/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3681 - acc: 0.8046 - val_loss: 1.1003 - val_acc: 0.5862
Epoch 33/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3657 - acc: 0.7960 - val_loss: 1.1087 - val_acc: 0.6207
Epoch 34/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3498 - acc: 0.8161 - val_loss: 1.1345 - val_acc: 0.6379
Epoch 35/35

348/348 [==============================] - 21s 61ms/step - loss: 0.3482 - acc: 0.8190 - val_loss: 1.1678 - val_acc: 0.5862
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.48151052e-01 5.18489219e-02]
 [5.09336948e-01 4.90663141e-01]
 [9.99999046e-01 9.31865884e-07]
 [9.72995877e-01 2.70041004e-02]
 [9.31239903e-01 6.87600598e-02]
 [5.14947474e-01 4.85052466e-01]
 [1.60014455e-03 9.98399913e-01]
 [1.00000000e+00 1.90558658e-09]
 [8.46938670e-01 1.53061330e-01]
 [9.99999642e-01 3.02120441e-07]
 [6.75479650e-01 3.24520379e-01]
 [4.50553805e-01 5.49446166e-01]
 [4.69702363e-01 5.30297637e-01]
 [5.14235914e-01 4.85764116e-01]
 [3.99822086e-01 6.00177944e-01]
 [9.99746621e-01 2.53329228e-04]
 [4.70794797e-01 5.29205263e-01]
 [5.59536695e-01 4.40463305e-01]
 [9.83295858e-01 1.67041793e-02]
 [6.64649997e-03 9.93353486e-01]
 [1.60014455e-03 9.98399913e-01]
 [4.70794797e-01 5.29205263e-01]
 [9.02172923e-03 9.90978241e-01]
 [9.31239903e-01 6.87600598e-02]
 [5.14235914e-01 4.85764116e-01]
 [4.79878813e-01 5.20121157e-01]
 [4.70035404e-01 5.29964566e-01]
 [9.55562174e-01 4.44378331e-02]
 [5.65965474e-01 4.34034497e-01]
 [3.99822086e-01 6.00177944e-01]
 [4.83344346e-01 5.16655624e-01]
 [5.29541671e-01 4.70458329e-01]
 [5.01089811e-01 4.98910218e-01]
 [5.65361500e-01 4.34638470e-01]
 [1.00000000e+00 3.58540824e-08]
 [2.85312608e-02 9.71468687e-01]
 [9.96383309e-01 3.61667294e-03]
 [4.50553805e-01 5.49446166e-01]
 [5.57332397e-01 4.42667633e-01]
 [4.83344346e-01 5.16655624e-01]
 [6.31476402e-01 3.68523657e-01]
 [4.47447211e-01 5.52552700e-01]
 [2.74559625e-06 9.99997258e-01]
 [9.99999046e-01 9.31865884e-07]
 [2.54104197e-01 7.45895803e-01]
 [5.65190315e-01 4.34809655e-01]
 [3.86107832e-01 6.13892138e-01]
 [5.70834875e-01 4.29165095e-01]
 [3.79702151e-01 6.20297849e-01]
 [9.96383309e-01 3.61667294e-03]
 [1.88458373e-03 9.98115420e-01]
 [9.96383309e-01 3.61667294e-03]
 [9.99746621e-01 2.53329228e-04]
 [7.14549124e-01 2.85450846e-01]
 [6.94831133e-01 3.05168837e-01]
 [9.62544262e-01 3.74557115e-02]
 [7.94797540e-01 2.05202505e-01]
 [4.83344346e-01 5.16655624e-01]
 [5.18712759e-01 4.81287241e-01]
 [9.31239903e-01 6.87600598e-02]
 [1.00000000e+00 1.90558658e-09]
 [4.79878813e-01 5.20121157e-01]
 [4.83344346e-01 5.16655624e-01]
 [6.91806734e-01 3.08193237e-01]
 [1.60014455e-03 9.98399913e-01]
 [3.25498223e-01 6.74501777e-01]
 [6.91806734e-01 3.08193237e-01]
 [3.79702151e-01 6.20297849e-01]
 [1.61976680e-01 8.38023365e-01]
 [5.22971392e-01 4.77028549e-01]
 [3.06864339e-03 9.96931314e-01]
 [4.90937531e-01 5.09062529e-01]
 [3.79702151e-01 6.20297849e-01]
 [7.86199991e-04 9.99213815e-01]
 [4.47447211e-01 5.52552700e-01]
 [5.70237517e-01 4.29762423e-01]
 [8.27923536e-01 1.72076508e-01]
 [1.00000000e+00 1.90558658e-09]
 [4.50553805e-01 5.49446166e-01]
 [4.50553805e-01 5.49446166e-01]
 [9.99746621e-01 2.53329228e-04]
 [8.83967996e-01 1.16031952e-01]
 [4.24232841e-01 5.75767100e-01]
 [5.70834875e-01 4.29165095e-01]
 [3.23785067e-01 6.76214874e-01]
 [9.99999642e-01 3.02120441e-07]
 [9.84109819e-01 1.58902183e-02]
 [5.29541671e-01 4.70458329e-01]
 [9.62868750e-01 3.71312797e-02]
 [2.57439762e-01 7.42560267e-01]
 [5.01089811e-01 4.98910218e-01]
 [4.90011036e-01 5.09988904e-01]
 [2.98554063e-01 7.01445937e-01]
 [9.86262679e-01 1.37373060e-02]
 [9.95244682e-01 4.75532422e-03]
 [9.72995877e-01 2.70041004e-02]
 [1.00000000e+00 1.90558658e-09]
 [4.90756214e-01 5.09243786e-01]
 [5.01089811e-01 4.98910218e-01]
 [5.47895133e-01 4.52104867e-01]
 [9.21089470e-01 7.89104551e-02]
 [2.74559625e-06 9.99997258e-01]
 [9.99999046e-01 9.31865884e-07]
 [6.87665880e-01 3.12334061e-01]
 [5.09320438e-01 4.90679592e-01]
 [9.99999642e-01 3.02120441e-07]
 [2.67667294e-01 7.32332706e-01]
 [5.43022931e-01 4.56977069e-01]
 [1.00000000e+00 1.33401405e-08]
 [6.94831133e-01 3.05168837e-01]
 [4.83344346e-01 5.16655624e-01]
 [7.97626376e-01 2.02373579e-01]
 [4.83344346e-01 5.16655624e-01]
 [1.00000000e+00 1.33401405e-08]
 [2.27345958e-01 7.72653997e-01]
 [4.01460528e-01 5.98539472e-01]]
[0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0
 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1
 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1
 0 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 17ms/step
Test loss: 1.2984808066795612
Test accuracy: 0.5344827586206896
[[36 22]
 [32 26]]

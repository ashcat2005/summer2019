Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:49.751733: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:49.860269: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:49.860517: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5616d738c770 executing computations on platform Host. Devices:
2019-11-07 09:04:49.860570: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/15

 58/348 [====>.........................] - ETA: 37s - loss: 0.6923 - acc: 0.4483
116/348 [=========>....................] - ETA: 22s - loss: 0.6914 - acc: 0.4741
174/348 [==============>...............] - ETA: 14s - loss: 0.6888 - acc: 0.5115
232/348 [===================>..........] - ETA: 9s - loss: 0.6973 - acc: 0.5086 
290/348 [========================>.....] - ETA: 4s - loss: 0.6979 - acc: 0.5000
348/348 [==============================] - 27s 78ms/step - loss: 0.6990 - acc: 0.5000 - val_loss: 0.7138 - val_acc: 0.5086
Epoch 2/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.7263 - acc: 0.4483
116/348 [=========>....................] - ETA: 13s - loss: 0.6999 - acc: 0.4569
174/348 [==============>...............] - ETA: 10s - loss: 0.6784 - acc: 0.5230
232/348 [===================>..........] - ETA: 6s - loss: 0.6631 - acc: 0.5388 
290/348 [========================>.....] - ETA: 3s - loss: 0.6540 - acc: 0.5517
348/348 [==============================] - 23s 65ms/step - loss: 0.6570 - acc: 0.5546 - val_loss: 0.6695 - val_acc: 0.5172
Epoch 3/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.6373 - acc: 0.5690
116/348 [=========>....................] - ETA: 13s - loss: 0.5979 - acc: 0.6983
174/348 [==============>...............] - ETA: 10s - loss: 0.5873 - acc: 0.7241
232/348 [===================>..........] - ETA: 6s - loss: 0.5670 - acc: 0.7371 
290/348 [========================>.....] - ETA: 3s - loss: 0.5693 - acc: 0.7345
348/348 [==============================] - 23s 65ms/step - loss: 0.5728 - acc: 0.7184 - val_loss: 0.6936 - val_acc: 0.5862
Epoch 4/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.4960 - acc: 0.8276
116/348 [=========>....................] - ETA: 13s - loss: 0.5012 - acc: 0.7845
174/348 [==============>...............] - ETA: 10s - loss: 0.5073 - acc: 0.7586
232/348 [===================>..........] - ETA: 6s - loss: 0.5098 - acc: 0.7414 
290/348 [========================>.....] - ETA: 3s - loss: 0.5050 - acc: 0.7448
348/348 [==============================] - 23s 65ms/step - loss: 0.4987 - acc: 0.7328 - val_loss: 0.8321 - val_acc: 0.5948
Epoch 5/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.4851 - acc: 0.6897
116/348 [=========>....................] - ETA: 13s - loss: 0.4810 - acc: 0.7155
174/348 [==============>...............] - ETA: 10s - loss: 0.4803 - acc: 0.7356
232/348 [===================>..........] - ETA: 6s - loss: 0.4816 - acc: 0.7155 
290/348 [========================>.....] - ETA: 3s - loss: 0.4617 - acc: 0.7069
348/348 [==============================] - 23s 65ms/step - loss: 0.4678 - acc: 0.7069 - val_loss: 0.9793 - val_acc: 0.5862
Epoch 6/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.4878 - acc: 0.7241
116/348 [=========>....................] - ETA: 13s - loss: 0.4085 - acc: 0.7759
174/348 [==============>...............] - ETA: 10s - loss: 0.4192 - acc: 0.7586
232/348 [===================>..........] - ETA: 6s - loss: 0.4172 - acc: 0.7629 
290/348 [========================>.....] - ETA: 3s - loss: 0.4286 - acc: 0.7483
348/348 [==============================] - 23s 65ms/step - loss: 0.4131 - acc: 0.7615 - val_loss: 1.1054 - val_acc: 0.6034
Epoch 7/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.3450 - acc: 0.8103
116/348 [=========>....................] - ETA: 13s - loss: 0.3760 - acc: 0.7931
174/348 [==============>...............] - ETA: 10s - loss: 0.3856 - acc: 0.7989
232/348 [===================>..........] - ETA: 6s - loss: 0.3662 - acc: 0.8103 
290/348 [========================>.....] - ETA: 3s - loss: 0.3813 - acc: 0.8034
348/348 [==============================] - 22s 64ms/step - loss: 0.3713 - acc: 0.8103 - val_loss: 1.2498 - val_acc: 0.6034
Epoch 8/15

 58/348 [====>.........................] - ETA: 16s - loss: 0.2955 - acc: 0.8448
116/348 [=========>....................] - ETA: 13s - loss: 0.2780 - acc: 0.8448
174/348 [==============>...............] - ETA: 10s - loss: 0.3318 - acc: 0.8161
232/348 [===================>..........] - ETA: 6s - loss: 0.3415 - acc: 0.8017 
290/348 [========================>.....] - ETA: 3s - loss: 0.3423 - acc: 0.8034
348/348 [==============================] - 22s 64ms/step - loss: 0.3298 - acc: 0.8161 - val_loss: 1.2999 - val_acc: 0.6293
Epoch 9/15

 58/348 [====>.........................] - ETA: 16s - loss: 0.3456 - acc: 0.8276
116/348 [=========>....................] - ETA: 13s - loss: 0.3379 - acc: 0.8362
174/348 [==============>...............] - ETA: 10s - loss: 0.3022 - acc: 0.8621
232/348 [===================>..........] - ETA: 6s - loss: 0.2912 - acc: 0.8664 
290/348 [========================>.....] - ETA: 3s - loss: 0.2783 - acc: 0.8655
348/348 [==============================] - 22s 64ms/step - loss: 0.2828 - acc: 0.8649 - val_loss: 1.3782 - val_acc: 0.6466
Epoch 10/15

 58/348 [====>.........................] - ETA: 16s - loss: 0.2472 - acc: 0.8621
116/348 [=========>....................] - ETA: 13s - loss: 0.2477 - acc: 0.8966
174/348 [==============>...............] - ETA: 10s - loss: 0.2479 - acc: 0.9023
232/348 [===================>..........] - ETA: 6s - loss: 0.2500 - acc: 0.8879 
290/348 [========================>.....] - ETA: 3s - loss: 0.2575 - acc: 0.8724
348/348 [==============================] - 22s 64ms/step - loss: 0.2496 - acc: 0.8736 - val_loss: 1.4593 - val_acc: 0.6552
Epoch 11/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.1801 - acc: 0.9310
116/348 [=========>....................] - ETA: 13s - loss: 0.1864 - acc: 0.9397
174/348 [==============>...............] - ETA: 10s - loss: 0.2193 - acc: 0.9138
232/348 [===================>..........] - ETA: 6s - loss: 0.2177 - acc: 0.9095 
290/348 [========================>.....] - ETA: 3s - loss: 0.2279 - acc: 0.9069
348/348 [==============================] - 23s 65ms/step - loss: 0.2253 - acc: 0.9138 - val_loss: 1.4280 - val_acc: 0.6897
Epoch 12/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.1964 - acc: 0.9483
116/348 [=========>....................] - ETA: 13s - loss: 0.1938 - acc: 0.9310
174/348 [==============>...............] - ETA: 10s - loss: 0.2022 - acc: 0.9195
232/348 [===================>..........] - ETA: 6s - loss: 0.2064 - acc: 0.9095 
290/348 [========================>.....] - ETA: 3s - loss: 0.1967 - acc: 0.9138
348/348 [==============================] - 22s 64ms/step - loss: 0.1941 - acc: 0.9253 - val_loss: 1.5138 - val_acc: 0.6638
Epoch 13/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.1728 - acc: 0.9138
116/348 [=========>....................] - ETA: 13s - loss: 0.1703 - acc: 0.9483
174/348 [==============>...............] - ETA: 10s - loss: 0.1720 - acc: 0.9425
232/348 [===================>..........] - ETA: 6s - loss: 0.1565 - acc: 0.9526 
290/348 [========================>.....] - ETA: 3s - loss: 0.1700 - acc: 0.9448
348/348 [==============================] - 22s 65ms/step - loss: 0.1708 - acc: 0.9511 - val_loss: 1.4164 - val_acc: 0.7328
Epoch 14/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.1352 - acc: 0.9655
116/348 [=========>....................] - ETA: 13s - loss: 0.1943 - acc: 0.9224
174/348 [==============>...............] - ETA: 10s - loss: 0.1784 - acc: 0.9310
232/348 [===================>..........] - ETA: 6s - loss: 0.1531 - acc: 0.9440 
290/348 [========================>.....] - ETA: 3s - loss: 0.1515 - acc: 0.9414
348/348 [==============================] - 23s 65ms/step - loss: 0.1506 - acc: 0.9425 - val_loss: 1.5752 - val_acc: 0.6552
Epoch 15/15

 58/348 [====>.........................] - ETA: 17s - loss: 0.1517 - acc: 0.9655
116/348 [=========>....................] - ETA: 13s - loss: 0.1362 - acc: 0.9655
174/348 [==============>...............] - ETA: 10s - loss: 0.1338 - acc: 0.9713
232/348 [===================>..........] - ETA: 6s - loss: 0.1276 - acc: 0.9784 
290/348 [========================>.....] - ETA: 3s - loss: 0.1192 - acc: 0.9793
348/348 [==============================] - 23s 65ms/step - loss: 0.1615 - acc: 0.9713 - val_loss: 1.2591 - val_acc: 0.7500
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.35046852e-01 6.49531931e-02]
 [3.89053702e-01 6.10946298e-01]
 [1.00000000e+00 2.08142304e-14]
 [9.94321704e-01 5.67825604e-03]
 [9.89849925e-01 1.01500442e-02]
 [3.34349602e-01 6.65650368e-01]
 [2.94050562e-10 1.00000000e+00]
 [1.00000000e+00 1.67829184e-30]
 [9.99462187e-01 5.37793851e-04]
 [1.00000000e+00 1.46186075e-17]
 [9.93317723e-01 6.68229675e-03]
 [8.76555741e-01 1.23444244e-01]
 [2.05700979e-01 7.94299006e-01]
 [3.02448124e-01 6.97551847e-01]
 [2.46778782e-02 9.75322127e-01]
 [1.00000000e+00 7.86952015e-09]
 [2.01404542e-01 7.98595428e-01]
 [4.63150561e-01 5.36849380e-01]
 [9.99845862e-01 1.54103938e-04]
 [9.15371004e-07 9.99999046e-01]
 [2.94050562e-10 1.00000000e+00]
 [2.01404542e-01 7.98595428e-01]
 [1.53409341e-09 1.00000000e+00]
 [9.89849925e-01 1.01500442e-02]
 [3.02448124e-01 6.97551847e-01]
 [6.50188327e-01 3.49811703e-01]
 [1.17777899e-01 8.82222116e-01]
 [9.99999762e-01 2.77272022e-07]
 [7.53338516e-01 2.46661454e-01]
 [2.46778782e-02 9.75322127e-01]
 [4.90012497e-01 5.09987414e-01]
 [1.77933082e-01 8.22066963e-01]
 [7.75957033e-02 9.22404230e-01]
 [2.00243264e-01 7.99756706e-01]
 [1.00000000e+00 2.65303069e-33]
 [1.72752734e-05 9.99982715e-01]
 [2.03219086e-01 7.96780884e-01]
 [8.76555741e-01 1.23444244e-01]
 [2.91537046e-01 7.08462954e-01]
 [4.90012497e-01 5.09987414e-01]
 [6.94491804e-01 3.05508226e-01]
 [1.46142887e-02 9.85385656e-01]
 [1.08852854e-03 9.98911500e-01]
 [1.00000000e+00 2.08142304e-14]
 [2.28815779e-01 7.71184206e-01]
 [1.73197873e-02 9.82680142e-01]
 [7.60341465e-01 2.39658549e-01]
 [7.80630171e-01 2.19369888e-01]
 [2.15921104e-02 9.78407860e-01]
 [2.03219086e-01 7.96780884e-01]
 [1.61352436e-04 9.99838591e-01]
 [2.03219086e-01 7.96780884e-01]
 [1.00000000e+00 7.86952015e-09]
 [9.99845862e-01 1.54106587e-04]
 [3.23741764e-01 6.76258206e-01]
 [9.99999881e-01 6.88106567e-08]
 [1.96566004e-02 9.80343342e-01]
 [4.90012497e-01 5.09987414e-01]
 [1.97780684e-01 8.02219272e-01]
 [9.89849925e-01 1.01500442e-02]
 [1.00000000e+00 1.67829184e-30]
 [6.50188327e-01 3.49811703e-01]
 [4.90012497e-01 5.09987414e-01]
 [8.99713755e-01 1.00286216e-01]
 [2.94050562e-10 1.00000000e+00]
 [3.15385498e-02 9.68461454e-01]
 [8.99713755e-01 1.00286216e-01]
 [2.15921104e-02 9.78407860e-01]
 [7.09478883e-03 9.92905200e-01]
 [8.39866281e-01 1.60133705e-01]
 [1.43865035e-21 1.00000000e+00]
 [2.71991909e-01 7.28008091e-01]
 [2.15921104e-02 9.78407860e-01]
 [6.40857233e-28 1.00000000e+00]
 [1.46142887e-02 9.85385656e-01]
 [5.17568290e-01 4.82431710e-01]
 [5.29285729e-01 4.70714301e-01]
 [1.00000000e+00 1.67829184e-30]
 [8.76555741e-01 1.23444244e-01]
 [8.76555741e-01 1.23444244e-01]
 [1.00000000e+00 7.86952015e-09]
 [8.18836153e-01 1.81163833e-01]
 [9.03954446e-01 9.60455686e-02]
 [7.80630171e-01 2.19369888e-01]
 [9.70749781e-02 9.02925074e-01]
 [1.00000000e+00 1.46186075e-17]
 [1.00000000e+00 3.98116310e-20]
 [1.77933082e-01 8.22066963e-01]
 [1.00000000e+00 1.31479385e-26]
 [2.83014417e-01 7.16985583e-01]
 [7.75957033e-02 9.22404230e-01]
 [3.13358396e-01 6.86641634e-01]
 [1.00000000e+00 1.04085116e-23]
 [1.00000000e+00 2.32869598e-26]
 [9.99996305e-01 3.67536290e-06]
 [9.94321704e-01 5.67825604e-03]
 [1.00000000e+00 1.67829184e-30]
 [1.98136479e-01 8.01863551e-01]
 [7.75957033e-02 9.22404230e-01]
 [5.54885745e-01 4.45114255e-01]
 [9.99026537e-01 9.73467249e-04]
 [1.08852854e-03 9.98911500e-01]
 [1.00000000e+00 2.08142304e-14]
 [7.90849090e-01 2.09150970e-01]
 [2.33240366e-01 7.66759634e-01]
 [1.00000000e+00 1.46186075e-17]
 [1.45880967e-05 9.99985456e-01]
 [5.93212903e-01 4.06787068e-01]
 [1.00000000e+00 1.15087526e-36]
 [3.23741764e-01 6.76258206e-01]
 [4.90012497e-01 5.09987414e-01]
 [9.57867384e-01 4.21326421e-02]
 [4.90012497e-01 5.09987414e-01]
 [1.00000000e+00 1.15087526e-36]
 [1.81004912e-01 8.18995118e-01]
 [6.04839623e-01 3.95160317e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1
 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1
 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1
 0 1 0 1 0]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 17ms/step
Test loss: 2.1247974428637275
Test accuracy: 0.6982758620689655
[[40 18]
 [17 41]]

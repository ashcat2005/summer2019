Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:05:02.475500: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:05:02.543701: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:05:02.543919: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5605ba929660 executing computations on platform Host. Devices:
2019-11-07 09:05:02.543970: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.25]
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 16s - loss: 0.6932 - acc: 0.4483
348/348 [==============================] - 29s 84ms/step - loss: 0.6906 - acc: 0.5086 - val_loss: 0.6605 - val_acc: 0.6034
Epoch 2/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6298 - acc: 0.7931
348/348 [==============================] - 23s 65ms/step - loss: 0.6214 - acc: 0.7443 - val_loss: 0.6547 - val_acc: 0.5345
Epoch 3/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5633 - acc: 0.6897
348/348 [==============================] - 22s 65ms/step - loss: 0.5851 - acc: 0.6408 - val_loss: 0.6644 - val_acc: 0.5603
Epoch 4/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5035 - acc: 0.7299
348/348 [==============================] - 22s 64ms/step - loss: 0.5267 - acc: 0.6925 - val_loss: 0.6871 - val_acc: 0.5603
Epoch 5/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4609 - acc: 0.7701
348/348 [==============================] - 22s 64ms/step - loss: 0.4754 - acc: 0.7672 - val_loss: 0.6941 - val_acc: 0.5862
Epoch 6/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4539 - acc: 0.7816
348/348 [==============================] - 22s 63ms/step - loss: 0.4539 - acc: 0.7816 - val_loss: 0.7281 - val_acc: 0.6207
Epoch 7/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4316 - acc: 0.7989
348/348 [==============================] - 22s 63ms/step - loss: 0.4188 - acc: 0.8218 - val_loss: 0.7962 - val_acc: 0.5862
Epoch 8/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3658 - acc: 0.8506
348/348 [==============================] - 22s 64ms/step - loss: 0.3940 - acc: 0.8017 - val_loss: 0.8635 - val_acc: 0.5776
Epoch 9/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3819 - acc: 0.8103
348/348 [==============================] - 22s 64ms/step - loss: 0.3613 - acc: 0.8448 - val_loss: 0.9090 - val_acc: 0.6466
Epoch 10/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3634 - acc: 0.8276
348/348 [==============================] - 22s 64ms/step - loss: 0.3395 - acc: 0.8563 - val_loss: 0.9846 - val_acc: 0.6466
Epoch 11/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3325 - acc: 0.8908
348/348 [==============================] - 22s 64ms/step - loss: 0.3152 - acc: 0.8707 - val_loss: 1.0575 - val_acc: 0.6207
Epoch 12/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3033 - acc: 0.8506
348/348 [==============================] - 22s 64ms/step - loss: 0.3012 - acc: 0.8649 - val_loss: 1.1006 - val_acc: 0.6466
Epoch 13/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2567 - acc: 0.9195
348/348 [==============================] - 22s 64ms/step - loss: 0.2760 - acc: 0.9167 - val_loss: 1.1707 - val_acc: 0.6466
Epoch 14/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2794 - acc: 0.9195
348/348 [==============================] - 22s 64ms/step - loss: 0.2606 - acc: 0.9224 - val_loss: 1.2133 - val_acc: 0.6466
Epoch 15/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2467 - acc: 0.9195
348/348 [==============================] - 22s 64ms/step - loss: 0.2439 - acc: 0.9109 - val_loss: 1.2528 - val_acc: 0.6466
Epoch 16/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2271 - acc: 0.9138
348/348 [==============================] - 22s 64ms/step - loss: 0.2270 - acc: 0.9310 - val_loss: 1.2859 - val_acc: 0.6724
Epoch 17/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2036 - acc: 0.9598
348/348 [==============================] - 22s 64ms/step - loss: 0.2125 - acc: 0.9540 - val_loss: 1.2950 - val_acc: 0.6638
Epoch 18/25

174/348 [==============>...............] - ETA: 9s - loss: 0.2156 - acc: 0.9080
348/348 [==============================] - 22s 64ms/step - loss: 0.2008 - acc: 0.9368 - val_loss: 1.3088 - val_acc: 0.6897
Epoch 19/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1618 - acc: 0.9713
348/348 [==============================] - 22s 64ms/step - loss: 0.1890 - acc: 0.9569 - val_loss: 1.3039 - val_acc: 0.6897
Epoch 20/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1976 - acc: 0.9195
348/348 [==============================] - 22s 64ms/step - loss: 0.1824 - acc: 0.9253 - val_loss: 1.3287 - val_acc: 0.6724
Epoch 21/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1563 - acc: 0.9885
348/348 [==============================] - 22s 64ms/step - loss: 0.1732 - acc: 0.9483 - val_loss: 1.3088 - val_acc: 0.6810
Epoch 22/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1717 - acc: 0.9598
348/348 [==============================] - 22s 64ms/step - loss: 0.1518 - acc: 0.9569 - val_loss: 1.3102 - val_acc: 0.6983
Epoch 23/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1509 - acc: 0.9195
348/348 [==============================] - 22s 63ms/step - loss: 0.1512 - acc: 0.9425 - val_loss: 1.3495 - val_acc: 0.6724
Epoch 24/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1482 - acc: 0.9885
348/348 [==============================] - 22s 64ms/step - loss: 0.1336 - acc: 0.9770 - val_loss: 1.3070 - val_acc: 0.7241
Epoch 25/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1209 - acc: 0.9655
348/348 [==============================] - 22s 64ms/step - loss: 0.1253 - acc: 0.9713 - val_loss: 1.3093 - val_acc: 0.7155
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.70506310e-01 2.94936821e-02]
 [2.98021793e-01 7.01978147e-01]
 [9.99999285e-01 7.42581562e-07]
 [9.69317317e-01 3.06826327e-02]
 [9.83931661e-01 1.60683636e-02]
 [3.72681350e-01 6.27318680e-01]
 [5.72685734e-04 9.99427319e-01]
 [1.00000000e+00 1.20261223e-11]
 [9.96684015e-01 3.31599172e-03]
 [1.00000000e+00 4.41619452e-09]
 [9.98155653e-01 1.84438354e-03]
 [6.50230408e-01 3.49769622e-01]
 [3.14083666e-01 6.85916424e-01]
 [5.09565055e-01 4.90434885e-01]
 [3.15297395e-01 6.84702635e-01]
 [9.99891400e-01 1.08597014e-04]
 [1.21621959e-01 8.78378034e-01]
 [5.14452159e-01 4.85547781e-01]
 [9.95970130e-01 4.02988959e-03]
 [3.31804273e-04 9.99668241e-01]
 [5.72685734e-04 9.99427319e-01]
 [1.21621959e-01 8.78378034e-01]
 [2.24931919e-05 9.99977469e-01]
 [9.83931661e-01 1.60683636e-02]
 [5.09565055e-01 4.90434885e-01]
 [4.80623513e-01 5.19376516e-01]
 [4.24067825e-01 5.75932205e-01]
 [9.99192774e-01 8.07263656e-04]
 [5.95530331e-01 4.04469669e-01]
 [3.15297395e-01 6.84702635e-01]
 [6.72019839e-01 3.27980161e-01]
 [4.12090123e-01 5.87909877e-01]
 [2.37021416e-01 7.62978613e-01]
 [4.18164223e-01 5.81835806e-01]
 [1.00000000e+00 3.21491802e-13]
 [8.00721289e-04 9.99199331e-01]
 [1.00000000e+00 9.17943090e-14]
 [6.50230408e-01 3.49769622e-01]
 [3.49261254e-01 6.50738716e-01]
 [6.72019839e-01 3.27980161e-01]
 [8.60606015e-01 1.39394015e-01]
 [9.31555256e-02 9.06844497e-01]
 [1.39241363e-09 1.00000000e+00]
 [9.99999285e-01 7.42581562e-07]
 [2.74793893e-01 7.25206077e-01]
 [9.81657028e-01 1.83429513e-02]
 [9.62033272e-01 3.79667431e-02]
 [6.57691896e-01 3.42308134e-01]
 [7.87991807e-02 9.21200871e-01]
 [1.00000000e+00 9.17943090e-14]
 [7.31858250e-04 9.99268115e-01]
 [1.00000000e+00 9.17943090e-14]
 [9.99891400e-01 1.08597014e-04]
 [9.93235648e-01 6.76440215e-03]
 [5.16557932e-01 4.83442098e-01]
 [9.96891081e-01 3.10894195e-03]
 [2.78285027e-01 7.21714973e-01]
 [6.72019839e-01 3.27980161e-01]
 [4.10505861e-01 5.89494109e-01]
 [9.83931661e-01 1.60683636e-02]
 [1.00000000e+00 1.20261223e-11]
 [4.80623513e-01 5.19376516e-01]
 [6.72019839e-01 3.27980161e-01]
 [6.68669999e-01 3.31330001e-01]
 [5.72685734e-04 9.99427319e-01]
 [1.10692838e-02 9.88930762e-01]
 [6.68669999e-01 3.31330001e-01]
 [7.87991807e-02 9.21200871e-01]
 [3.10657829e-01 6.89342201e-01]
 [5.92992187e-01 4.07007813e-01]
 [2.80672516e-08 1.00000000e+00]
 [3.20302695e-01 6.79697335e-01]
 [7.87991807e-02 9.21200871e-01]
 [5.83455035e-16 1.00000000e+00]
 [9.31555256e-02 9.06844497e-01]
 [4.71475184e-01 5.28524816e-01]
 [4.51235920e-01 5.48764110e-01]
 [1.00000000e+00 1.20261223e-11]
 [6.50230408e-01 3.49769622e-01]
 [6.50230408e-01 3.49769622e-01]
 [9.99891400e-01 1.08597014e-04]
 [7.92544425e-01 2.07455605e-01]
 [5.04484951e-01 4.95514989e-01]
 [6.57691896e-01 3.42308134e-01]
 [1.91203415e-01 8.08796525e-01]
 [1.00000000e+00 4.41619452e-09]
 [1.00000000e+00 5.00559247e-11]
 [4.12090123e-01 5.87909877e-01]
 [1.00000000e+00 4.59979624e-12]
 [3.25677454e-01 6.74322546e-01]
 [2.37021416e-01 7.62978613e-01]
 [3.05019468e-01 6.94980502e-01]
 [9.04955387e-01 9.50446054e-02]
 [9.99993205e-01 6.74520106e-06]
 [9.98074651e-01 1.92532106e-03]
 [9.69317317e-01 3.06826327e-02]
 [1.00000000e+00 1.20261223e-11]
 [4.37079549e-01 5.62920392e-01]
 [2.37021416e-01 7.62978613e-01]
 [4.69317347e-01 5.30682564e-01]
 [8.96228015e-01 1.03772044e-01]
 [1.39241363e-09 1.00000000e+00]
 [9.99999285e-01 7.42581562e-07]
 [5.69059730e-01 4.30940270e-01]
 [4.94812399e-01 5.05187631e-01]
 [1.00000000e+00 4.41619452e-09]
 [4.37949831e-03 9.95620549e-01]
 [5.20786583e-01 4.79213476e-01]
 [1.00000000e+00 2.17378128e-19]
 [5.16557932e-01 4.83442098e-01]
 [6.72019839e-01 3.27980161e-01]
 [8.61796319e-01 1.38203710e-01]
 [6.72019839e-01 3.27980161e-01]
 [1.00000000e+00 2.17378128e-19]
 [3.46800923e-01 6.53199017e-01]
 [4.41619009e-01 5.58381021e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0
 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1
 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0
 0 0 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 19ms/step
Test loss: 2.110436936904644
Test accuracy: 0.7068965517241379
[[45 13]
 [21 37]]

Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:22.152002: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:22.155952: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:22.156057: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55eddda0a9c0 executing computations on platform Host. Devices:
2019-11-07 09:04:22.156078: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.25]
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 11s - loss: 0.6924 - acc: 0.5517
348/348 [==============================] - 24s 68ms/step - loss: 0.6925 - acc: 0.5316 - val_loss: 0.6813 - val_acc: 0.5431
Epoch 2/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6741 - acc: 0.6034
348/348 [==============================] - 21s 62ms/step - loss: 0.6770 - acc: 0.5517 - val_loss: 0.6831 - val_acc: 0.5086
Epoch 3/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6467 - acc: 0.5632
348/348 [==============================] - 21s 61ms/step - loss: 0.6472 - acc: 0.5489 - val_loss: 0.7215 - val_acc: 0.5690
Epoch 4/25

174/348 [==============>...............] - ETA: 9s - loss: 0.7339 - acc: 0.4943
348/348 [==============================] - 21s 62ms/step - loss: 0.6950 - acc: 0.4914 - val_loss: 0.6748 - val_acc: 0.5000
Epoch 5/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6139 - acc: 0.5805
348/348 [==============================] - 21s 61ms/step - loss: 0.6328 - acc: 0.5690 - val_loss: 0.6644 - val_acc: 0.5086
Epoch 6/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5844 - acc: 0.7069
348/348 [==============================] - 21s 61ms/step - loss: 0.6039 - acc: 0.6322 - val_loss: 0.6847 - val_acc: 0.5948
Epoch 7/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6146 - acc: 0.6207
348/348 [==============================] - 21s 61ms/step - loss: 0.5842 - acc: 0.6667 - val_loss: 0.6867 - val_acc: 0.5086
Epoch 8/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5393 - acc: 0.7011
348/348 [==============================] - 21s 61ms/step - loss: 0.5648 - acc: 0.6667 - val_loss: 0.7892 - val_acc: 0.5000
Epoch 9/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6202 - acc: 0.5862
348/348 [==============================] - 21s 62ms/step - loss: 0.5831 - acc: 0.6178 - val_loss: 0.7105 - val_acc: 0.5948
Epoch 10/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4967 - acc: 0.7701
348/348 [==============================] - 21s 61ms/step - loss: 0.5173 - acc: 0.7241 - val_loss: 0.7484 - val_acc: 0.5862
Epoch 11/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5265 - acc: 0.6552
348/348 [==============================] - 21s 61ms/step - loss: 0.6148 - acc: 0.6121 - val_loss: 1.0925 - val_acc: 0.6034
Epoch 12/25

174/348 [==============>...............] - ETA: 9s - loss: 0.8297 - acc: 0.6092
348/348 [==============================] - 21s 61ms/step - loss: 0.6556 - acc: 0.6954 - val_loss: 1.3155 - val_acc: 0.5172
Epoch 13/25

174/348 [==============>...............] - ETA: 9s - loss: 1.0104 - acc: 0.5345
348/348 [==============================] - 21s 61ms/step - loss: 0.9461 - acc: 0.5460 - val_loss: 0.8291 - val_acc: 0.5948
Epoch 14/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4858 - acc: 0.7299
348/348 [==============================] - 21s 61ms/step - loss: 0.7061 - acc: 0.6580 - val_loss: 0.8626 - val_acc: 0.5862
Epoch 15/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4799 - acc: 0.7414
348/348 [==============================] - 21s 61ms/step - loss: 0.6381 - acc: 0.6839 - val_loss: 1.2130 - val_acc: 0.5086
Epoch 16/25

174/348 [==============>...............] - ETA: 9s - loss: 0.8214 - acc: 0.5747
348/348 [==============================] - 21s 62ms/step - loss: 0.6216 - acc: 0.6868 - val_loss: 1.0862 - val_acc: 0.5948
Epoch 17/25

174/348 [==============>...............] - ETA: 9s - loss: 0.7598 - acc: 0.6034
348/348 [==============================] - 21s 61ms/step - loss: 0.9157 - acc: 0.5661 - val_loss: 1.0438 - val_acc: 0.5948
Epoch 18/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6030 - acc: 0.6724
348/348 [==============================] - 21s 62ms/step - loss: 0.5235 - acc: 0.7213 - val_loss: 1.1411 - val_acc: 0.5172
Epoch 19/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5795 - acc: 0.6782
348/348 [==============================] - 21s 61ms/step - loss: 0.5582 - acc: 0.7011 - val_loss: 1.0005 - val_acc: 0.5862
Epoch 20/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4224 - acc: 0.7126
348/348 [==============================] - 21s 61ms/step - loss: 0.4739 - acc: 0.7011 - val_loss: 1.5789 - val_acc: 0.5086
Epoch 21/25

174/348 [==============>...............] - ETA: 9s - loss: 0.9242 - acc: 0.6207
348/348 [==============================] - 21s 61ms/step - loss: 1.0594 - acc: 0.6149 - val_loss: 1.0746 - val_acc: 0.5431
Epoch 22/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4912 - acc: 0.7356
348/348 [==============================] - 21s 61ms/step - loss: 0.5959 - acc: 0.6897 - val_loss: 2.4723 - val_acc: 0.6034
Epoch 23/25

174/348 [==============>...............] - ETA: 9s - loss: 1.9556 - acc: 0.5747
348/348 [==============================] - 21s 62ms/step - loss: 1.2114 - acc: 0.6437 - val_loss: 1.4433 - val_acc: 0.5000
Epoch 24/25

174/348 [==============>...............] - ETA: 9s - loss: 0.7153 - acc: 0.6724
348/348 [==============================] - 21s 61ms/step - loss: 1.2559 - acc: 0.6494 - val_loss: 1.1386 - val_acc: 0.5690
Epoch 25/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4504 - acc: 0.7586
348/348 [==============================] - 21s 61ms/step - loss: 0.9988 - acc: 0.6810 - val_loss: 2.0750 - val_acc: 0.6034
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[1.12190127e-01 8.87809813e-01]
 [3.26386169e-02 9.67361391e-01]
 [1.00000000e+00 3.45051419e-08]
 [2.89956689e-01 7.10043311e-01]
 [9.12851021e-02 9.08714890e-01]
 [1.31868571e-02 9.86813068e-01]
 [2.33861465e-05 9.99976635e-01]
 [1.00000000e+00 1.75036555e-10]
 [3.23427208e-02 9.67657208e-01]
 [9.99998927e-01 1.09581151e-06]
 [9.44152381e-03 9.90558505e-01]
 [1.19831869e-02 9.88016844e-01]
 [1.05197513e-02 9.89480257e-01]
 [1.23356655e-02 9.87664402e-01]
 [7.21167261e-03 9.92788374e-01]
 [1.00000000e+00 3.18965228e-08]
 [1.01682851e-02 9.89831686e-01]
 [1.58081278e-02 9.84191895e-01]
 [5.35953045e-01 4.64047045e-01]
 [9.90130793e-05 9.99900937e-01]
 [2.33861465e-05 9.99976635e-01]
 [1.01682851e-02 9.89831686e-01]
 [9.07587673e-05 9.99909282e-01]
 [9.12851021e-02 9.08714890e-01]
 [1.23356655e-02 9.87664402e-01]
 [1.14994086e-02 9.88500595e-01]
 [1.07287643e-02 9.89271283e-01]
 [2.94675708e-01 7.05324292e-01]
 [1.45807285e-02 9.85419273e-01]
 [7.21167261e-03 9.92788374e-01]
 [3.98068922e-03 9.96019304e-01]
 [1.38520543e-02 9.86147881e-01]
 [1.30176106e-02 9.86982405e-01]
 [1.47257838e-02 9.85274196e-01]
 [9.99997377e-01 2.63175139e-06]
 [9.22769017e-04 9.99077201e-01]
 [8.41872469e-02 9.15812790e-01]
 [1.19831869e-02 9.88016844e-01]
 [1.51985520e-02 9.84801471e-01]
 [3.98068922e-03 9.96019304e-01]
 [9.40627325e-03 9.90593731e-01]
 [1.23925349e-02 9.87607419e-01]
 [7.58532348e-10 1.00000000e+00]
 [1.00000000e+00 3.45051419e-08]
 [2.05567270e-03 9.97944295e-01]
 [1.77765451e-02 9.82223511e-01]
 [6.36499096e-03 9.93635058e-01]
 [1.46720009e-02 9.85328019e-01]
 [6.95035793e-03 9.93049622e-01]
 [8.41872469e-02 9.15812790e-01]
 [4.73960927e-06 9.99995232e-01]
 [8.41872469e-02 9.15812790e-01]
 [1.00000000e+00 3.18965228e-08]
 [1.16005111e-02 9.88399506e-01]
 [2.53910553e-02 9.74608898e-01]
 [1.11129999e-01 8.88870060e-01]
 [2.51384620e-02 9.74861622e-01]
 [3.98068922e-03 9.96019304e-01]
 [1.28325587e-02 9.87167418e-01]
 [9.12851021e-02 9.08714890e-01]
 [1.00000000e+00 1.75036555e-10]
 [1.14994086e-02 9.88500595e-01]
 [3.98068922e-03 9.96019304e-01]
 [2.88326796e-02 9.71167266e-01]
 [2.33861465e-05 9.99976635e-01]
 [3.78267802e-02 9.62173223e-01]
 [2.88326796e-02 9.71167266e-01]
 [6.95035793e-03 9.93049622e-01]
 [2.20009638e-03 9.97799933e-01]
 [5.32946968e-03 9.94670570e-01]
 [5.49251912e-04 9.99450743e-01]
 [1.19108744e-02 9.88089144e-01]
 [6.95035793e-03 9.93049622e-01]
 [8.22399961e-05 9.99917746e-01]
 [1.23925349e-02 9.87607419e-01]
 [1.56375859e-02 9.84362483e-01]
 [6.39164448e-02 9.36083555e-01]
 [1.00000000e+00 1.75036555e-10]
 [1.19831869e-02 9.88016844e-01]
 [1.19831869e-02 9.88016844e-01]
 [1.00000000e+00 3.18965228e-08]
 [4.58311886e-02 9.54168856e-01]
 [7.71757355e-03 9.92282450e-01]
 [1.46720009e-02 9.85328019e-01]
 [2.78059696e-03 9.97219443e-01]
 [9.99998927e-01 1.09581151e-06]
 [3.77984308e-02 9.62201595e-01]
 [1.38520543e-02 9.86147881e-01]
 [1.31301414e-02 9.86869872e-01]
 [2.13937764e-03 9.97860610e-01]
 [1.30176106e-02 9.86982405e-01]
 [1.13421604e-02 9.88657892e-01]
 [1.80489069e-03 9.98195112e-01]
 [6.16536140e-01 3.83463830e-01]
 [4.04347926e-01 5.95652103e-01]
 [2.89956689e-01 7.10043311e-01]
 [1.00000000e+00 1.75036555e-10]
 [1.20830517e-02 9.87916887e-01]
 [1.30176106e-02 9.86982405e-01]
 [1.48648005e-02 9.85135138e-01]
 [2.36466955e-02 9.76353288e-01]
 [7.58532348e-10 1.00000000e+00]
 [1.00000000e+00 3.45051419e-08]
 [2.52483487e-02 9.74751592e-01]
 [1.16406055e-02 9.88359332e-01]
 [9.99998927e-01 1.09581151e-06]
 [9.92481969e-03 9.90075171e-01]
 [1.36512220e-02 9.86348748e-01]
 [1.00000000e+00 2.09523978e-11]
 [2.53910553e-02 9.74608898e-01]
 [3.98068922e-03 9.96019304e-01]
 [3.91631983e-02 9.60836828e-01]
 [3.98068922e-03 9.96019304e-01]
 [1.00000000e+00 2.09523978e-11]
 [1.45718444e-03 9.98542786e-01]
 [5.41010918e-03 9.94589925e-01]]
[1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1
 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1
 1 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 17ms/step
Test loss: 1.9323119952760894
Test accuracy: 0.637931032427426
[[17 41]
 [ 1 57]]

Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:47.066246: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:47.163899: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:47.164150: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x563db5b4bbc0 executing computations on platform Host. Devices:
2019-11-07 09:04:47.164205: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/35

348/348 [==============================] - 16s 45ms/step - loss: 0.6924 - acc: 0.5086 - val_loss: 0.6898 - val_acc: 0.5172
Epoch 2/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6839 - acc: 0.5805 - val_loss: 0.6884 - val_acc: 0.5000
Epoch 3/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6752 - acc: 0.6724 - val_loss: 0.6883 - val_acc: 0.5345
Epoch 4/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6671 - acc: 0.6810 - val_loss: 0.6892 - val_acc: 0.5345
Epoch 5/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6596 - acc: 0.6954 - val_loss: 0.6909 - val_acc: 0.5259
Epoch 6/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6526 - acc: 0.6667 - val_loss: 0.6934 - val_acc: 0.5431
Epoch 7/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6459 - acc: 0.6724 - val_loss: 0.6964 - val_acc: 0.5517
Epoch 8/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6396 - acc: 0.6695 - val_loss: 0.7001 - val_acc: 0.5690
Epoch 9/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6335 - acc: 0.6839 - val_loss: 0.7046 - val_acc: 0.5345
Epoch 10/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6276 - acc: 0.6810 - val_loss: 0.7099 - val_acc: 0.5603
Epoch 11/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6218 - acc: 0.6954 - val_loss: 0.7160 - val_acc: 0.5690
Epoch 12/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6161 - acc: 0.6925 - val_loss: 0.7230 - val_acc: 0.5690
Epoch 13/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6105 - acc: 0.6954 - val_loss: 0.7307 - val_acc: 0.5690
Epoch 14/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6049 - acc: 0.6897 - val_loss: 0.7393 - val_acc: 0.5776
Epoch 15/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5994 - acc: 0.6839 - val_loss: 0.7487 - val_acc: 0.5862
Epoch 16/35

348/348 [==============================] - 13s 36ms/step - loss: 0.5939 - acc: 0.6868 - val_loss: 0.7590 - val_acc: 0.5862
Epoch 17/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5884 - acc: 0.7069 - val_loss: 0.7701 - val_acc: 0.5862
Epoch 18/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5829 - acc: 0.7069 - val_loss: 0.7816 - val_acc: 0.5690
Epoch 19/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5775 - acc: 0.7155 - val_loss: 0.7931 - val_acc: 0.5690
Epoch 20/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5721 - acc: 0.7126 - val_loss: 0.8045 - val_acc: 0.5690
Epoch 21/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5667 - acc: 0.7184 - val_loss: 0.8164 - val_acc: 0.5776
Epoch 22/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5613 - acc: 0.7184 - val_loss: 0.8288 - val_acc: 0.5776
Epoch 23/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5560 - acc: 0.7155 - val_loss: 0.8413 - val_acc: 0.5862
Epoch 24/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5507 - acc: 0.7126 - val_loss: 0.8535 - val_acc: 0.5948
Epoch 25/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5454 - acc: 0.7213 - val_loss: 0.8658 - val_acc: 0.5948
Epoch 26/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5402 - acc: 0.7184 - val_loss: 0.8787 - val_acc: 0.5862
Epoch 27/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5350 - acc: 0.7184 - val_loss: 0.8916 - val_acc: 0.5862
Epoch 28/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5299 - acc: 0.7213 - val_loss: 0.9039 - val_acc: 0.5862
Epoch 29/35

348/348 [==============================] - 13s 36ms/step - loss: 0.5249 - acc: 0.7126 - val_loss: 0.9167 - val_acc: 0.5862
Epoch 30/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5199 - acc: 0.7155 - val_loss: 0.9299 - val_acc: 0.5862
Epoch 31/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5150 - acc: 0.7126 - val_loss: 0.9425 - val_acc: 0.5776
Epoch 32/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5101 - acc: 0.7155 - val_loss: 0.9561 - val_acc: 0.5776
Epoch 33/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5053 - acc: 0.7155 - val_loss: 0.9697 - val_acc: 0.6034
Epoch 34/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5005 - acc: 0.7270 - val_loss: 0.9831 - val_acc: 0.6034
Epoch 35/35

348/348 [==============================] - 13s 37ms/step - loss: 0.4958 - acc: 0.7299 - val_loss: 0.9983 - val_acc: 0.6034
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[6.5831369e-01 3.4168634e-01]
 [4.8846313e-01 5.1153690e-01]
 [8.7351942e-01 1.2648059e-01]
 [7.9170692e-01 2.0829308e-01]
 [4.9834022e-01 5.0165975e-01]
 [4.9049810e-01 5.0950187e-01]
 [4.4772878e-02 9.5522714e-01]
 [9.5368439e-01 4.6315555e-02]
 [5.2356350e-01 4.7643653e-01]
 [9.7865272e-01 2.1347223e-02]
 [5.3991765e-01 4.6008238e-01]
 [4.6700311e-01 5.3299689e-01]
 [4.5294878e-01 5.4705131e-01]
 [4.3335128e-01 5.6664866e-01]
 [3.8970464e-01 6.1029530e-01]
 [9.9663782e-01 3.3621739e-03]
 [2.6317677e-01 7.3682314e-01]
 [5.1628542e-01 4.8371455e-01]
 [6.2403148e-01 3.7596849e-01]
 [3.0624792e-01 6.9375205e-01]
 [4.4772878e-02 9.5522714e-01]
 [2.6317677e-01 7.3682314e-01]
 [2.7663311e-01 7.2336686e-01]
 [4.9834022e-01 5.0165975e-01]
 [4.3335128e-01 5.6664866e-01]
 [3.8756388e-01 6.1243612e-01]
 [4.6710250e-01 5.3289747e-01]
 [3.4061655e-01 6.5938348e-01]
 [4.9739456e-01 5.0260550e-01]
 [3.8970464e-01 6.1029530e-01]
 [4.8290637e-01 5.1709360e-01]
 [4.8834458e-01 5.1165533e-01]
 [4.3785524e-01 5.6214476e-01]
 [5.1759088e-01 4.8240906e-01]
 [9.8407125e-01 1.5928749e-02]
 [4.3730000e-01 5.6269991e-01]
 [9.6531389e-03 9.9034685e-01]
 [4.6700311e-01 5.3299689e-01]
 [4.9829265e-01 5.0170726e-01]
 [4.8290637e-01 5.1709360e-01]
 [2.7685171e-01 7.2314835e-01]
 [4.5502338e-01 5.4497665e-01]
 [1.2831531e-06 9.9999869e-01]
 [8.7351942e-01 1.2648059e-01]
 [3.9198872e-01 6.0801125e-01]
 [7.5444990e-01 2.4555010e-01]
 [3.9287812e-01 6.0712188e-01]
 [4.4096747e-01 5.5903256e-01]
 [3.9173922e-01 6.0826075e-01]
 [9.6531389e-03 9.9034685e-01]
 [2.6877098e-02 9.7312284e-01]
 [9.6531389e-03 9.9034685e-01]
 [9.9663782e-01 3.3621739e-03]
 [5.9314966e-01 4.0685034e-01]
 [5.5281836e-01 4.4718164e-01]
 [9.0188822e-03 9.9098116e-01]
 [5.6799114e-01 4.3200889e-01]
 [4.8290637e-01 5.1709360e-01]
 [4.7711855e-01 5.2288139e-01]
 [4.9834022e-01 5.0165975e-01]
 [9.5368439e-01 4.6315555e-02]
 [3.8756388e-01 6.1243612e-01]
 [4.8290637e-01 5.1709360e-01]
 [6.0164833e-01 3.9835161e-01]
 [4.4772878e-02 9.5522714e-01]
 [7.1897751e-01 2.8102252e-01]
 [6.0164833e-01 3.9835161e-01]
 [3.9173922e-01 6.0826075e-01]
 [4.4806930e-01 5.5193067e-01]
 [5.3283268e-01 4.6716723e-01]
 [9.2022943e-01 7.9770602e-02]
 [4.4936249e-01 5.5063742e-01]
 [3.9173922e-01 6.0826075e-01]
 [1.0000000e+00 5.5798037e-09]
 [4.5502338e-01 5.4497665e-01]
 [4.8411688e-01 5.1588309e-01]
 [6.1015797e-01 3.8984203e-01]
 [9.5368439e-01 4.6315555e-02]
 [4.6700311e-01 5.3299689e-01]
 [4.6700311e-01 5.3299689e-01]
 [9.9663782e-01 3.3621739e-03]
 [5.4110664e-01 4.5889336e-01]
 [3.4024701e-01 6.5975296e-01]
 [4.4096747e-01 5.5903256e-01]
 [3.9594829e-01 6.0405165e-01]
 [9.7865272e-01 2.1347223e-02]
 [2.5053334e-05 9.9997497e-01]
 [4.8834458e-01 5.1165533e-01]
 [8.5992663e-04 9.9914002e-01]
 [3.9539388e-01 6.0460609e-01]
 [4.3785524e-01 5.6214476e-01]
 [4.5640162e-01 5.4359835e-01]
 [9.9688776e-02 9.0031117e-01]
 [9.4245201e-01 5.7547979e-02]
 [9.1302115e-01 8.6978838e-02]
 [7.9170692e-01 2.0829308e-01]
 [9.5368439e-01 4.6315555e-02]
 [4.5531261e-01 5.4468739e-01]
 [4.3785524e-01 5.6214476e-01]
 [4.7694662e-01 5.2305335e-01]
 [9.3264413e-01 6.7355819e-02]
 [1.2831531e-06 9.9999869e-01]
 [8.7351942e-01 1.2648059e-01]
 [5.4097098e-01 4.5902899e-01]
 [4.8160109e-01 5.1839888e-01]
 [9.7865272e-01 2.1347223e-02]
 [4.3436363e-01 5.6563640e-01]
 [4.8920503e-01 5.1079488e-01]
 [9.9995613e-01 4.3835997e-05]
 [5.5281836e-01 4.4718164e-01]
 [4.8290637e-01 5.1709360e-01]
 [5.5769867e-01 4.4230136e-01]
 [4.8290637e-01 5.1709360e-01]
 [9.9995613e-01 4.3835997e-05]
 [4.2716718e-01 5.7283282e-01]
 [4.1845500e-01 5.8154500e-01]]
[0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1
 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0
 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1
 0 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 1s 12ms/step
Test loss: 0.857754111289978
Test accuracy: 0.637931032427426
[[29 29]
 [13 45]]

Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:05.467278: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:05.471183: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:05.471289: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b8028cccf0 executing computations on platform Host. Devices:
2019-11-07 09:04:05.471308: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.25]
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 6s - loss: 0.6943 - acc: 0.5115
348/348 [==============================] - 13s 38ms/step - loss: 0.6940 - acc: 0.5000 - val_loss: 0.6909 - val_acc: 0.5259
Epoch 2/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6797 - acc: 0.6379
348/348 [==============================] - 13s 36ms/step - loss: 0.6795 - acc: 0.6264 - val_loss: 0.6905 - val_acc: 0.5172
Epoch 3/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6702 - acc: 0.6954
348/348 [==============================] - 13s 37ms/step - loss: 0.6677 - acc: 0.6552 - val_loss: 0.6915 - val_acc: 0.4569
Epoch 4/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6557 - acc: 0.6897
348/348 [==============================] - 13s 37ms/step - loss: 0.6565 - acc: 0.6868 - val_loss: 0.6943 - val_acc: 0.5172
Epoch 5/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6529 - acc: 0.6609
348/348 [==============================] - 13s 37ms/step - loss: 0.6469 - acc: 0.6868 - val_loss: 0.6986 - val_acc: 0.5431
Epoch 6/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6460 - acc: 0.6552
348/348 [==============================] - 13s 37ms/step - loss: 0.6373 - acc: 0.6954 - val_loss: 0.7050 - val_acc: 0.5603
Epoch 7/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6284 - acc: 0.6897
348/348 [==============================] - 13s 37ms/step - loss: 0.6282 - acc: 0.6954 - val_loss: 0.7141 - val_acc: 0.5431
Epoch 8/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6206 - acc: 0.6609
348/348 [==============================] - 13s 37ms/step - loss: 0.6194 - acc: 0.7069 - val_loss: 0.7248 - val_acc: 0.5690
Epoch 9/25

174/348 [==============>...............] - ETA: 5s - loss: 0.6076 - acc: 0.7241
348/348 [==============================] - 13s 37ms/step - loss: 0.6109 - acc: 0.7040 - val_loss: 0.7357 - val_acc: 0.5948
Epoch 10/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5914 - acc: 0.7586
348/348 [==============================] - 13s 37ms/step - loss: 0.6024 - acc: 0.7184 - val_loss: 0.7489 - val_acc: 0.5948
Epoch 11/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5953 - acc: 0.7299
348/348 [==============================] - 13s 37ms/step - loss: 0.5932 - acc: 0.7069 - val_loss: 0.7660 - val_acc: 0.5948
Epoch 12/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5764 - acc: 0.7299
348/348 [==============================] - 13s 37ms/step - loss: 0.5843 - acc: 0.7299 - val_loss: 0.7860 - val_acc: 0.6034
Epoch 13/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5710 - acc: 0.7241
348/348 [==============================] - 13s 37ms/step - loss: 0.5755 - acc: 0.7241 - val_loss: 0.8063 - val_acc: 0.5948
Epoch 14/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5739 - acc: 0.6954
348/348 [==============================] - 13s 37ms/step - loss: 0.5664 - acc: 0.7241 - val_loss: 0.8248 - val_acc: 0.5948
Epoch 15/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5865 - acc: 0.6667
348/348 [==============================] - 13s 37ms/step - loss: 0.5570 - acc: 0.7126 - val_loss: 0.8433 - val_acc: 0.6034
Epoch 16/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5387 - acc: 0.7529
348/348 [==============================] - 13s 37ms/step - loss: 0.5500 - acc: 0.7213 - val_loss: 0.8647 - val_acc: 0.6034
Epoch 17/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5383 - acc: 0.7529
348/348 [==============================] - 13s 37ms/step - loss: 0.5401 - acc: 0.7270 - val_loss: 0.8925 - val_acc: 0.5862
Epoch 18/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5291 - acc: 0.7184
348/348 [==============================] - 13s 37ms/step - loss: 0.5329 - acc: 0.7213 - val_loss: 0.9226 - val_acc: 0.5517
Epoch 19/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5313 - acc: 0.7184
348/348 [==============================] - 13s 37ms/step - loss: 0.5251 - acc: 0.7213 - val_loss: 0.9394 - val_acc: 0.6034
Epoch 20/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5399 - acc: 0.7126
348/348 [==============================] - 13s 37ms/step - loss: 0.5160 - acc: 0.7299 - val_loss: 0.9585 - val_acc: 0.5862
Epoch 21/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5100 - acc: 0.7069
348/348 [==============================] - 13s 37ms/step - loss: 0.5085 - acc: 0.7356 - val_loss: 0.9853 - val_acc: 0.6034
Epoch 22/25

174/348 [==============>...............] - ETA: 5s - loss: 0.4969 - acc: 0.7529
348/348 [==============================] - 13s 37ms/step - loss: 0.5015 - acc: 0.7414 - val_loss: 1.0127 - val_acc: 0.6034
Epoch 23/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5234 - acc: 0.7356
348/348 [==============================] - 13s 37ms/step - loss: 0.4931 - acc: 0.7529 - val_loss: 1.0412 - val_acc: 0.5862
Epoch 24/25

174/348 [==============>...............] - ETA: 5s - loss: 0.4630 - acc: 0.7931
348/348 [==============================] - 13s 37ms/step - loss: 0.4889 - acc: 0.7672 - val_loss: 1.0606 - val_acc: 0.5862
Epoch 25/25

174/348 [==============>...............] - ETA: 5s - loss: 0.5036 - acc: 0.7701
348/348 [==============================] - 13s 37ms/step - loss: 0.4800 - acc: 0.7701 - val_loss: 1.0695 - val_acc: 0.6034
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[6.4779931e-01 3.5220069e-01]
 [4.6880120e-01 5.3119880e-01]
 [9.0593612e-01 9.4063818e-02]
 [7.7774674e-01 2.2225335e-01]
 [4.9668962e-01 5.0331038e-01]
 [4.6125260e-01 5.3874737e-01]
 [2.5248760e-02 9.7475123e-01]
 [9.2174065e-01 7.8259289e-02]
 [5.4478627e-01 4.5521376e-01]
 [9.7269690e-01 2.7303148e-02]
 [5.1477212e-01 4.8522785e-01]
 [4.4721678e-01 5.5278319e-01]
 [4.2674592e-01 5.7325405e-01]
 [4.0025678e-01 5.9974325e-01]
 [3.5281181e-01 6.4718825e-01]
 [9.9558544e-01 4.4145794e-03]
 [2.4808688e-01 7.5191319e-01]
 [5.0078666e-01 4.9921340e-01]
 [5.7477218e-01 4.2522782e-01]
 [2.3506856e-01 7.6493138e-01]
 [2.5248760e-02 9.7475123e-01]
 [2.4808688e-01 7.5191319e-01]
 [2.4907437e-01 7.5092560e-01]
 [4.9668962e-01 5.0331038e-01]
 [4.0025678e-01 5.9974325e-01]
 [3.4235874e-01 6.5764123e-01]
 [4.3761107e-01 5.6238884e-01]
 [3.5217455e-01 6.4782536e-01]
 [4.7730014e-01 5.2269983e-01]
 [3.5281181e-01 6.4718825e-01]
 [4.6387875e-01 5.3612119e-01]
 [4.6994692e-01 5.3005308e-01]
 [4.0128809e-01 5.9871191e-01]
 [5.0828272e-01 4.9171722e-01]
 [9.8386973e-01 1.6130313e-02]
 [4.3023187e-01 5.6976807e-01]
 [2.5057171e-03 9.9749434e-01]
 [4.4721678e-01 5.5278319e-01]
 [4.8070195e-01 5.1929802e-01]
 [4.6387875e-01 5.3612119e-01]
 [2.6778391e-01 7.3221618e-01]
 [4.2077598e-01 5.7922405e-01]
 [1.0073816e-08 1.0000000e+00]
 [9.0593612e-01 9.4063818e-02]
 [3.7761042e-01 6.2238961e-01]
 [6.9335240e-01 3.0664757e-01]
 [3.4646410e-01 6.5353590e-01]
 [4.1566023e-01 5.8433980e-01]
 [3.5496774e-01 6.4503223e-01]
 [2.5057171e-03 9.9749434e-01]
 [1.6240189e-02 9.8375976e-01]
 [2.5057171e-03 9.9749434e-01]
 [9.9558544e-01 4.4145794e-03]
 [6.2603939e-01 3.7396058e-01]
 [5.1843566e-01 4.8156437e-01]
 [3.3495193e-03 9.9665052e-01]
 [5.5828339e-01 4.4171661e-01]
 [4.6387875e-01 5.3612119e-01]
 [4.5828435e-01 5.4171568e-01]
 [4.9668962e-01 5.0331038e-01]
 [9.2174065e-01 7.8259289e-02]
 [3.4235874e-01 6.5764123e-01]
 [4.6387875e-01 5.3612119e-01]
 [6.1717474e-01 3.8282523e-01]
 [2.5248760e-02 9.7475123e-01]
 [6.9272149e-01 3.0727851e-01]
 [6.1717474e-01 3.8282523e-01]
 [3.5496774e-01 6.4503223e-01]
 [4.3152004e-01 5.6847990e-01]
 [5.1490271e-01 4.8509729e-01]
 [6.3236082e-01 3.6763912e-01]
 [4.2405820e-01 5.7594186e-01]
 [3.5496774e-01 6.4503223e-01]
 [1.0000000e+00 1.6456497e-10]
 [4.2077598e-01 5.7922405e-01]
 [4.6204406e-01 5.3795594e-01]
 [6.1018926e-01 3.8981074e-01]
 [9.2174065e-01 7.8259289e-02]
 [4.4721678e-01 5.5278319e-01]
 [4.4721678e-01 5.5278319e-01]
 [9.9558544e-01 4.4145794e-03]
 [5.5124092e-01 4.4875905e-01]
 [3.2669079e-01 6.7330915e-01]
 [4.1566023e-01 5.8433980e-01]
 [3.7346029e-01 6.2653977e-01]
 [9.7269690e-01 2.7303148e-02]
 [2.0632217e-06 9.9999797e-01]
 [4.6994692e-01 5.3005308e-01]
 [2.0879884e-04 9.9979120e-01]
 [3.7911296e-01 6.2088704e-01]
 [4.0128809e-01 5.9871191e-01]
 [4.3703690e-01 5.6296307e-01]
 [2.5113512e-02 9.7488654e-01]
 [9.7331661e-01 2.6683394e-02]
 [8.7042427e-01 1.2957570e-01]
 [7.7774674e-01 2.2225335e-01]
 [9.2174065e-01 7.8259289e-02]
 [4.1895261e-01 5.8104742e-01]
 [4.0128809e-01 5.9871191e-01]
 [4.5410004e-01 5.4589993e-01]
 [9.2882699e-01 7.1173012e-02]
 [1.0073816e-08 1.0000000e+00]
 [9.0593612e-01 9.4063818e-02]
 [5.3365272e-01 4.6634731e-01]
 [4.6611014e-01 5.3388983e-01]
 [9.7269690e-01 2.7303148e-02]
 [4.1915989e-01 5.8084011e-01]
 [4.6974102e-01 5.3025907e-01]
 [9.9999380e-01 6.1963929e-06]
 [5.1843566e-01 4.8156437e-01]
 [4.6387875e-01 5.3612119e-01]
 [5.3360587e-01 4.6639413e-01]
 [4.6387875e-01 5.3612119e-01]
 [9.9999380e-01 6.1963929e-06]
 [3.8547730e-01 6.1452264e-01]
 [4.0507960e-01 5.9492034e-01]]
[0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1
 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0
 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1
 0 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 1s 12ms/step
Test loss: 0.9158662023215458
Test accuracy: 0.637931032427426
[[29 29]
 [13 45]]

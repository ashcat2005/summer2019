Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:05.563887: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:05.567916: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:05.568044: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55f2eff8e100 executing computations on platform Host. Devices:
2019-11-07 09:04:05.568064: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/35

348/348 [==============================] - 21s 60ms/step - loss: 0.6936 - acc: 0.5029 - val_loss: 0.6961 - val_acc: 0.4569
Epoch 2/35

348/348 [==============================] - 20s 56ms/step - loss: 0.6899 - acc: 0.5862 - val_loss: 0.6957 - val_acc: 0.4483
Epoch 3/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6860 - acc: 0.6063 - val_loss: 0.6959 - val_acc: 0.4655
Epoch 4/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6821 - acc: 0.6063 - val_loss: 0.6968 - val_acc: 0.4914
Epoch 5/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6782 - acc: 0.6034 - val_loss: 0.6986 - val_acc: 0.4914
Epoch 6/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6744 - acc: 0.5862 - val_loss: 0.7013 - val_acc: 0.4741
Epoch 7/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6709 - acc: 0.5920 - val_loss: 0.7051 - val_acc: 0.4828
Epoch 8/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6675 - acc: 0.5977 - val_loss: 0.7099 - val_acc: 0.4828
Epoch 9/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6642 - acc: 0.6006 - val_loss: 0.7154 - val_acc: 0.4828
Epoch 10/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6611 - acc: 0.5891 - val_loss: 0.7217 - val_acc: 0.5000
Epoch 11/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6582 - acc: 0.5718 - val_loss: 0.7286 - val_acc: 0.5086
Epoch 12/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6553 - acc: 0.5805 - val_loss: 0.7359 - val_acc: 0.5086
Epoch 13/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6524 - acc: 0.5862 - val_loss: 0.7436 - val_acc: 0.5000
Epoch 14/35

348/348 [==============================] - 20s 56ms/step - loss: 0.6496 - acc: 0.6034 - val_loss: 0.7514 - val_acc: 0.5000
Epoch 15/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6468 - acc: 0.6063 - val_loss: 0.7592 - val_acc: 0.5000
Epoch 16/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6439 - acc: 0.5977 - val_loss: 0.7667 - val_acc: 0.5000
Epoch 17/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6411 - acc: 0.5920 - val_loss: 0.7739 - val_acc: 0.5000
Epoch 18/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6381 - acc: 0.5920 - val_loss: 0.7808 - val_acc: 0.5000
Epoch 19/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6351 - acc: 0.5920 - val_loss: 0.7874 - val_acc: 0.5259
Epoch 20/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6320 - acc: 0.6178 - val_loss: 0.7939 - val_acc: 0.5431
Epoch 21/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6289 - acc: 0.6207 - val_loss: 0.8002 - val_acc: 0.5603
Epoch 22/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6258 - acc: 0.6408 - val_loss: 0.8063 - val_acc: 0.5603
Epoch 23/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6227 - acc: 0.6494 - val_loss: 0.8123 - val_acc: 0.5690
Epoch 24/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6195 - acc: 0.6523 - val_loss: 0.8183 - val_acc: 0.5776
Epoch 25/35

348/348 [==============================] - 19s 55ms/step - loss: 0.6163 - acc: 0.6523 - val_loss: 0.8242 - val_acc: 0.5862
Epoch 26/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6131 - acc: 0.6466 - val_loss: 0.8300 - val_acc: 0.6034
Epoch 27/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6098 - acc: 0.6552 - val_loss: 0.8357 - val_acc: 0.6034
Epoch 28/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6064 - acc: 0.6667 - val_loss: 0.8415 - val_acc: 0.6121
Epoch 29/35

348/348 [==============================] - 19s 56ms/step - loss: 0.6029 - acc: 0.6695 - val_loss: 0.8475 - val_acc: 0.6207
Epoch 30/35

348/348 [==============================] - 19s 56ms/step - loss: 0.5993 - acc: 0.6695 - val_loss: 0.8538 - val_acc: 0.6207
Epoch 31/35

348/348 [==============================] - 19s 56ms/step - loss: 0.5955 - acc: 0.6695 - val_loss: 0.8608 - val_acc: 0.6034
Epoch 32/35

348/348 [==============================] - 19s 56ms/step - loss: 0.5916 - acc: 0.6839 - val_loss: 0.8683 - val_acc: 0.6034
Epoch 33/35

348/348 [==============================] - 19s 56ms/step - loss: 0.5875 - acc: 0.6954 - val_loss: 0.8760 - val_acc: 0.6293
Epoch 34/35

348/348 [==============================] - 19s 56ms/step - loss: 0.5831 - acc: 0.7011 - val_loss: 0.8843 - val_acc: 0.6293
Epoch 35/35

348/348 [==============================] - 19s 55ms/step - loss: 0.5786 - acc: 0.7040 - val_loss: 0.8930 - val_acc: 0.6379
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[6.1979914e-01 3.8020086e-01]
 [4.8843640e-01 5.1156354e-01]
 [7.2516584e-01 2.7483416e-01]
 [7.6160383e-01 2.3839618e-01]
 [6.3522255e-01 3.6477745e-01]
 [4.6698305e-01 5.3301698e-01]
 [2.4239020e-01 7.5760978e-01]
 [8.3991820e-01 1.6008186e-01]
 [5.3467613e-01 4.6532384e-01]
 [7.4344879e-01 2.5655115e-01]
 [4.2498004e-01 5.7501996e-01]
 [5.0977361e-01 4.9022636e-01]
 [4.4395387e-01 5.5604613e-01]
 [4.7565138e-01 5.2434862e-01]
 [4.3676776e-01 5.6323230e-01]
 [9.7769153e-01 2.2308517e-02]
 [4.0257147e-01 5.9742856e-01]
 [4.7842130e-01 5.2157873e-01]
 [7.1492916e-01 2.8507081e-01]
 [4.8475888e-01 5.1524109e-01]
 [2.4239020e-01 7.5760978e-01]
 [4.0257147e-01 5.9742856e-01]
 [3.6734003e-01 6.3265997e-01]
 [6.3522255e-01 3.6477745e-01]
 [4.7565138e-01 5.2434862e-01]
 [4.7174045e-01 5.2825958e-01]
 [4.7679672e-01 5.2320325e-01]
 [3.8151827e-01 6.1848170e-01]
 [4.6181095e-01 5.3818905e-01]
 [4.3676776e-01 5.6323230e-01]
 [4.3845513e-01 5.6154484e-01]
 [4.7132164e-01 5.2867836e-01]
 [4.8859277e-01 5.1140720e-01]
 [4.6748447e-01 5.3251559e-01]
 [9.9401724e-01 5.9827999e-03]
 [6.0055494e-01 3.9944506e-01]
 [9.9388677e-01 6.1132745e-03]
 [5.0977361e-01 4.9022636e-01]
 [4.5899919e-01 5.4100084e-01]
 [4.3845513e-01 5.6154484e-01]
 [3.5843584e-01 6.4156419e-01]
 [4.6488878e-01 5.3511125e-01]
 [8.5883939e-01 1.4116062e-01]
 [7.2516584e-01 2.7483416e-01]
 [3.7731886e-01 6.2268114e-01]
 [3.6950663e-01 6.3049346e-01]
 [4.3904841e-01 5.6095159e-01]
 [4.5060146e-01 5.4939854e-01]
 [4.6019834e-01 5.3980172e-01]
 [9.9388677e-01 6.1132745e-03]
 [1.7024755e-02 9.8297518e-01]
 [9.9388677e-01 6.1132745e-03]
 [9.7769153e-01 2.2308517e-02]
 [4.3334332e-01 5.6665665e-01]
 [4.8897794e-01 5.1102197e-01]
 [4.4176891e-01 5.5823112e-01]
 [4.9422193e-01 5.0577807e-01]
 [4.3845513e-01 5.6154484e-01]
 [4.8340380e-01 5.1659620e-01]
 [6.3522255e-01 3.6477745e-01]
 [8.3991820e-01 1.6008186e-01]
 [4.7174045e-01 5.2825958e-01]
 [4.3845513e-01 5.6154484e-01]
 [4.7906959e-01 5.2093041e-01]
 [2.4239020e-01 7.5760978e-01]
 [6.8945646e-01 3.1054351e-01]
 [4.7906959e-01 5.2093041e-01]
 [4.6019834e-01 5.3980172e-01]
 [4.5141846e-01 5.4858160e-01]
 [3.5416552e-01 6.4583445e-01]
 [9.9986219e-01 1.3782250e-04]
 [4.5543379e-01 5.4456621e-01]
 [4.6019834e-01 5.3980172e-01]
 [9.9540222e-01 4.5977533e-03]
 [4.6488878e-01 5.3511125e-01]
 [4.7234145e-01 5.2765864e-01]
 [5.1549470e-01 4.8450527e-01]
 [8.3991820e-01 1.6008186e-01]
 [5.0977361e-01 4.9022636e-01]
 [5.0977361e-01 4.9022636e-01]
 [9.7769153e-01 2.2308517e-02]
 [4.6144152e-01 5.3855842e-01]
 [3.9309669e-01 6.0690331e-01]
 [4.5060146e-01 5.4939854e-01]
 [3.2384795e-01 6.7615211e-01]
 [7.4344879e-01 2.5655115e-01]
 [1.7159791e-03 9.9828404e-01]
 [4.7132164e-01 5.2867836e-01]
 [9.2132217e-01 7.8677796e-02]
 [3.7895724e-01 6.2104273e-01]
 [4.8859277e-01 5.1140720e-01]
 [4.4932598e-01 5.5067402e-01]
 [5.3641105e-01 4.6358895e-01]
 [8.3018357e-01 1.6981645e-01]
 [8.0362219e-01 1.9637777e-01]
 [7.6160383e-01 2.3839618e-01]
 [8.3991820e-01 1.6008186e-01]
 [4.7287360e-01 5.2712637e-01]
 [4.8859277e-01 5.1140720e-01]
 [4.6787658e-01 5.3212339e-01]
 [6.6398865e-01 3.3601138e-01]
 [8.5883939e-01 1.4116062e-01]
 [7.2516584e-01 2.7483416e-01]
 [4.5605522e-01 5.4394472e-01]
 [4.6955073e-01 5.3044933e-01]
 [7.4344879e-01 2.5655115e-01]
 [5.2579552e-01 4.7420445e-01]
 [4.7369802e-01 5.2630198e-01]
 [9.9856865e-01 1.4313762e-03]
 [4.8897794e-01 5.1102197e-01]
 [4.3845513e-01 5.6154484e-01]
 [4.9321103e-01 5.0678903e-01]
 [4.3845513e-01 5.6154484e-01]
 [9.9856865e-01 1.4313762e-03]
 [4.3245459e-01 5.6754547e-01]
 [3.8822138e-01 6.1177862e-01]]
[0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0
 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0
 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1
 1 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 15ms/step
Test loss: 0.7266174739804762
Test accuracy: 0.6724137889927831
[[32 26]
 [12 46]]

Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:03:34.987726: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:03:34.991592: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:03:34.991695: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55a8fb8ec8c0 executing computations on platform Host. Devices:
2019-11-07 09:03:34.991714: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/15

 58/348 [====>.........................] - ETA: 49s - loss: 0.6941 - acc: 0.4138
116/348 [=========>....................] - ETA: 29s - loss: 0.6929 - acc: 0.4655
174/348 [==============>...............] - ETA: 19s - loss: 0.6915 - acc: 0.4770
232/348 [===================>..........] - ETA: 12s - loss: 0.6879 - acc: 0.4957
290/348 [========================>.....] - ETA: 5s - loss: 0.6905 - acc: 0.4966 
348/348 [==============================] - 36s 104ms/step - loss: 0.6854 - acc: 0.5201 - val_loss: 0.6564 - val_acc: 0.6207
Epoch 2/15

 58/348 [====>.........................] - ETA: 24s - loss: 0.5895 - acc: 0.8966
116/348 [=========>....................] - ETA: 19s - loss: 0.5853 - acc: 0.8966
174/348 [==============>...............] - ETA: 14s - loss: 0.5944 - acc: 0.8333
232/348 [===================>..........] - ETA: 9s - loss: 0.5855 - acc: 0.8017 
290/348 [========================>.....] - ETA: 4s - loss: 0.5741 - acc: 0.7862
348/348 [==============================] - 31s 88ms/step - loss: 0.5940 - acc: 0.7385 - val_loss: 0.6591 - val_acc: 0.5690
Epoch 3/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.4895 - acc: 0.7759
116/348 [=========>....................] - ETA: 19s - loss: 0.5790 - acc: 0.6724
174/348 [==============>...............] - ETA: 14s - loss: 0.5573 - acc: 0.6897
232/348 [===================>..........] - ETA: 9s - loss: 0.5470 - acc: 0.7026 
290/348 [========================>.....] - ETA: 4s - loss: 0.5321 - acc: 0.7207
348/348 [==============================] - 30s 87ms/step - loss: 0.5297 - acc: 0.7184 - val_loss: 0.7477 - val_acc: 0.5345
Epoch 4/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.4517 - acc: 0.7586
116/348 [=========>....................] - ETA: 18s - loss: 0.4673 - acc: 0.7241
174/348 [==============>...............] - ETA: 14s - loss: 0.4749 - acc: 0.7184
232/348 [===================>..........] - ETA: 9s - loss: 0.4695 - acc: 0.7328 
290/348 [========================>.....] - ETA: 4s - loss: 0.4588 - acc: 0.7379
348/348 [==============================] - 30s 87ms/step - loss: 0.4592 - acc: 0.7356 - val_loss: 0.8083 - val_acc: 0.6552
Epoch 5/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.4241 - acc: 0.8793
116/348 [=========>....................] - ETA: 18s - loss: 0.3963 - acc: 0.8793
174/348 [==============>...............] - ETA: 13s - loss: 0.3996 - acc: 0.8736
232/348 [===================>..........] - ETA: 9s - loss: 0.3907 - acc: 0.8664 
290/348 [========================>.....] - ETA: 4s - loss: 0.3869 - acc: 0.8552
348/348 [==============================] - 30s 86ms/step - loss: 0.3968 - acc: 0.8333 - val_loss: 0.9465 - val_acc: 0.5690
Epoch 6/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.4050 - acc: 0.7586
116/348 [=========>....................] - ETA: 18s - loss: 0.3906 - acc: 0.8103
174/348 [==============>...............] - ETA: 14s - loss: 0.3599 - acc: 0.8391
232/348 [===================>..........] - ETA: 9s - loss: 0.3578 - acc: 0.8534 
290/348 [========================>.....] - ETA: 4s - loss: 0.3535 - acc: 0.8517
348/348 [==============================] - 30s 86ms/step - loss: 0.3442 - acc: 0.8448 - val_loss: 1.0634 - val_acc: 0.5862
Epoch 7/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.3702 - acc: 0.7931
116/348 [=========>....................] - ETA: 18s - loss: 0.3005 - acc: 0.8707
174/348 [==============>...............] - ETA: 13s - loss: 0.3151 - acc: 0.8678
232/348 [===================>..........] - ETA: 9s - loss: 0.3058 - acc: 0.8621 
290/348 [========================>.....] - ETA: 4s - loss: 0.2939 - acc: 0.8793
348/348 [==============================] - 30s 86ms/step - loss: 0.2993 - acc: 0.8793 - val_loss: 1.1459 - val_acc: 0.6293
Epoch 8/15

 58/348 [====>.........................] - ETA: 22s - loss: 0.3085 - acc: 0.8793
116/348 [=========>....................] - ETA: 18s - loss: 0.2811 - acc: 0.8879
174/348 [==============>...............] - ETA: 13s - loss: 0.2629 - acc: 0.9080
232/348 [===================>..........] - ETA: 9s - loss: 0.2636 - acc: 0.9009 
290/348 [========================>.....] - ETA: 4s - loss: 0.2569 - acc: 0.9034
348/348 [==============================] - 30s 86ms/step - loss: 0.2601 - acc: 0.8966 - val_loss: 1.2257 - val_acc: 0.6638
Epoch 9/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.2293 - acc: 0.9310
116/348 [=========>....................] - ETA: 18s - loss: 0.2463 - acc: 0.9397
174/348 [==============>...............] - ETA: 13s - loss: 0.2406 - acc: 0.9483
232/348 [===================>..........] - ETA: 9s - loss: 0.2345 - acc: 0.9483 
290/348 [========================>.....] - ETA: 4s - loss: 0.2241 - acc: 0.9517
348/348 [==============================] - 30s 87ms/step - loss: 0.2224 - acc: 0.9425 - val_loss: 1.2504 - val_acc: 0.6810
Epoch 10/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.1810 - acc: 0.9310
116/348 [=========>....................] - ETA: 18s - loss: 0.1864 - acc: 0.9397
174/348 [==============>...............] - ETA: 13s - loss: 0.1883 - acc: 0.9368
232/348 [===================>..........] - ETA: 9s - loss: 0.1900 - acc: 0.9310 
290/348 [========================>.....] - ETA: 4s - loss: 0.1978 - acc: 0.9345
348/348 [==============================] - 30s 86ms/step - loss: 0.1889 - acc: 0.9454 - val_loss: 1.2988 - val_acc: 0.6810
Epoch 11/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.1665 - acc: 0.9828
116/348 [=========>....................] - ETA: 18s - loss: 0.1528 - acc: 0.9914
174/348 [==============>...............] - ETA: 13s - loss: 0.1579 - acc: 0.9598
232/348 [===================>..........] - ETA: 9s - loss: 0.1648 - acc: 0.9526 
290/348 [========================>.....] - ETA: 4s - loss: 0.1583 - acc: 0.9586
348/348 [==============================] - 30s 86ms/step - loss: 0.1603 - acc: 0.9540 - val_loss: 1.3209 - val_acc: 0.6983
Epoch 12/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.1258 - acc: 1.0000
116/348 [=========>....................] - ETA: 18s - loss: 0.1556 - acc: 0.9655
174/348 [==============>...............] - ETA: 13s - loss: 0.1461 - acc: 0.9598
232/348 [===================>..........] - ETA: 9s - loss: 0.1330 - acc: 0.9698 
290/348 [========================>.....] - ETA: 4s - loss: 0.1482 - acc: 0.9552
348/348 [==============================] - 30s 86ms/step - loss: 0.1391 - acc: 0.9569 - val_loss: 1.2907 - val_acc: 0.7500
Epoch 13/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.1120 - acc: 0.9828
116/348 [=========>....................] - ETA: 18s - loss: 0.1204 - acc: 0.9828
174/348 [==============>...............] - ETA: 13s - loss: 0.1277 - acc: 0.9885
232/348 [===================>..........] - ETA: 9s - loss: 0.1204 - acc: 0.9914 
290/348 [========================>.....] - ETA: 4s - loss: 0.1127 - acc: 0.9931
348/348 [==============================] - 30s 86ms/step - loss: 0.1089 - acc: 0.9943 - val_loss: 1.3254 - val_acc: 0.7069
Epoch 14/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.1147 - acc: 1.0000
116/348 [=========>....................] - ETA: 18s - loss: 0.1119 - acc: 0.9828
174/348 [==============>...............] - ETA: 13s - loss: 0.0997 - acc: 0.9885
232/348 [===================>..........] - ETA: 9s - loss: 0.0987 - acc: 0.9914 
290/348 [========================>.....] - ETA: 4s - loss: 0.0918 - acc: 0.9897
348/348 [==============================] - 30s 86ms/step - loss: 0.0941 - acc: 0.9799 - val_loss: 1.3090 - val_acc: 0.8017
Epoch 15/15

 58/348 [====>.........................] - ETA: 23s - loss: 0.1250 - acc: 0.9655
116/348 [=========>....................] - ETA: 18s - loss: 0.0990 - acc: 0.9828
174/348 [==============>...............] - ETA: 13s - loss: 0.0909 - acc: 0.9885
232/348 [===================>..........] - ETA: 9s - loss: 0.0934 - acc: 0.9914 
290/348 [========================>.....] - ETA: 4s - loss: 0.0859 - acc: 0.9931
348/348 [==============================] - 30s 86ms/step - loss: 0.0801 - acc: 0.9943 - val_loss: 1.3181 - val_acc: 0.8103
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.75498497e-01 2.45015509e-02]
 [1.87610015e-01 8.12389970e-01]
 [1.00000000e+00 9.20312093e-09]
 [9.68505085e-01 3.14949527e-02]
 [9.80335236e-01 1.96648184e-02]
 [2.94532895e-01 7.05467105e-01]
 [6.11041614e-05 9.99938846e-01]
 [1.00000000e+00 3.86906598e-13]
 [9.98797297e-01 1.20270671e-03]
 [1.00000000e+00 6.30463650e-12]
 [9.98951316e-01 1.04863709e-03]
 [8.87457550e-01 1.12542436e-01]
 [2.47553110e-01 7.52446949e-01]
 [3.90692294e-01 6.09307706e-01]
 [3.61451358e-01 6.38548613e-01]
 [9.99397993e-01 6.02020998e-04]
 [5.73698133e-02 9.42630172e-01]
 [4.17809635e-01 5.82190394e-01]
 [9.92366135e-01 7.63390446e-03]
 [9.01885724e-05 9.99909759e-01]
 [6.11041614e-05 9.99938846e-01]
 [5.73698133e-02 9.42630172e-01]
 [3.75418772e-08 1.00000000e+00]
 [9.80335236e-01 1.96648184e-02]
 [3.90692294e-01 6.09307706e-01]
 [4.94037539e-01 5.05962431e-01]
 [2.83390105e-01 7.16609895e-01]
 [9.99117196e-01 8.82767374e-04]
 [5.44878721e-01 4.55121309e-01]
 [3.61451358e-01 6.38548613e-01]
 [8.43645275e-01 1.56354696e-01]
 [2.69169658e-01 7.30830312e-01]
 [8.77990723e-02 9.12200928e-01]
 [3.14717799e-01 6.85282230e-01]
 [1.00000000e+00 8.16231851e-17]
 [6.77544813e-05 9.99932289e-01]
 [1.00000000e+00 1.96200121e-16]
 [8.87457550e-01 1.12542436e-01]
 [1.97676882e-01 8.02323103e-01]
 [8.43645275e-01 1.56354696e-01]
 [9.60534811e-01 3.94652374e-02]
 [2.25613620e-02 9.77438629e-01]
 [1.56118146e-10 1.00000000e+00]
 [1.00000000e+00 9.20312093e-09]
 [1.19422756e-01 8.80577266e-01]
 [9.97588873e-01 2.41110264e-03]
 [9.71022069e-01 2.89779603e-02]
 [6.79777741e-01 3.20222318e-01]
 [9.18254629e-03 9.90817428e-01]
 [1.00000000e+00 1.96200121e-16]
 [8.56707265e-06 9.99991417e-01]
 [1.00000000e+00 1.96200121e-16]
 [9.99397993e-01 6.02020998e-04]
 [9.98484433e-01 1.51553657e-03]
 [3.26889098e-01 6.73110962e-01]
 [9.73966420e-01 2.60336511e-02]
 [3.25903669e-02 9.67409611e-01]
 [8.43645275e-01 1.56354696e-01]
 [2.79215574e-01 7.20784485e-01]
 [9.80335236e-01 1.96648184e-02]
 [1.00000000e+00 3.86906598e-13]
 [4.94037539e-01 5.05962431e-01]
 [8.43645275e-01 1.56354696e-01]
 [7.58696616e-01 2.41303340e-01]
 [6.11041614e-05 9.99938846e-01]
 [3.92647879e-03 9.96073484e-01]
 [7.58696616e-01 2.41303340e-01]
 [9.18254629e-03 9.90817428e-01]
 [4.60357249e-01 5.39642811e-01]
 [4.10838068e-01 5.89161932e-01]
 [3.78712284e-10 1.00000000e+00]
 [2.33722001e-01 7.66278028e-01]
 [9.18254629e-03 9.90817428e-01]
 [5.47179130e-26 1.00000000e+00]
 [2.25613620e-02 9.77438629e-01]
 [3.58529687e-01 6.41470253e-01]
 [1.35919571e-01 8.64080429e-01]
 [1.00000000e+00 3.86906598e-13]
 [8.87457550e-01 1.12542436e-01]
 [8.87457550e-01 1.12542436e-01]
 [9.99397993e-01 6.02020998e-04]
 [7.18386114e-01 2.81613916e-01]
 [6.44599438e-01 3.55400622e-01]
 [6.79777741e-01 3.20222318e-01]
 [5.45264930e-02 9.45473552e-01]
 [1.00000000e+00 6.30463650e-12]
 [1.00000000e+00 2.57019124e-13]
 [2.69169658e-01 7.30830312e-01]
 [1.00000000e+00 2.58657723e-17]
 [1.54152215e-01 8.45847845e-01]
 [8.77990723e-02 9.12200928e-01]
 [1.94440067e-01 8.05559933e-01]
 [1.00000000e+00 6.89141277e-09]
 [1.00000000e+00 4.27379174e-11]
 [9.99801576e-01 1.98485286e-04]
 [9.68505085e-01 3.14949527e-02]
 [1.00000000e+00 3.86906598e-13]
 [2.98221022e-01 7.01778948e-01]
 [8.77990723e-02 9.12200928e-01]
 [4.11270261e-01 5.88729799e-01]
 [7.28696525e-01 2.71303475e-01]
 [1.56118146e-10 1.00000000e+00]
 [1.00000000e+00 9.20312093e-09]
 [5.09423852e-01 4.90576118e-01]
 [3.26907337e-01 6.73092663e-01]
 [1.00000000e+00 6.30463650e-12]
 [3.64735693e-04 9.99635220e-01]
 [4.45694804e-01 5.54305196e-01]
 [1.00000000e+00 1.68278656e-29]
 [3.26889098e-01 6.73110962e-01]
 [8.43645275e-01 1.56354696e-01]
 [8.96089852e-01 1.03910178e-01]
 [8.43645275e-01 1.56354696e-01]
 [1.00000000e+00 1.68278656e-29]
 [5.28947175e-01 4.71052885e-01]
 [3.08457404e-01 6.91542566e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0
 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1
 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0
 0 0 0 0 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 19ms/step
Test loss: 2.2715942243049883
Test accuracy: 0.7413793124001602
[[44 14]
 [16 42]]

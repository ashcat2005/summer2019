Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:05:02.744504: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:05:02.748286: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:05:02.748376: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5588ffa9ab50 executing computations on platform Host. Devices:
2019-11-07 09:05:02.748397: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.25]
Train on 348 samples, validate on 116 samples
Epoch 1/35

348/348 [==============================] - 27s 79ms/step - loss: 0.6933 - acc: 0.5057 - val_loss: 0.6738 - val_acc: 0.5776
Epoch 2/35

348/348 [==============================] - 20s 58ms/step - loss: 0.6576 - acc: 0.7213 - val_loss: 0.6570 - val_acc: 0.5948
Epoch 3/35

348/348 [==============================] - 20s 58ms/step - loss: 0.6184 - acc: 0.7960 - val_loss: 0.6522 - val_acc: 0.5603
Epoch 4/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5784 - acc: 0.7069 - val_loss: 0.6495 - val_acc: 0.5862
Epoch 5/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5428 - acc: 0.7816 - val_loss: 0.6807 - val_acc: 0.5172
Epoch 6/35

348/348 [==============================] - 20s 58ms/step - loss: 0.5202 - acc: 0.7011 - val_loss: 0.6712 - val_acc: 0.6121
Epoch 7/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4936 - acc: 0.7299 - val_loss: 0.6782 - val_acc: 0.6810
Epoch 8/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4568 - acc: 0.8707 - val_loss: 0.7139 - val_acc: 0.5603
Epoch 9/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4418 - acc: 0.7615 - val_loss: 0.7256 - val_acc: 0.5431
Epoch 10/35

348/348 [==============================] - 20s 58ms/step - loss: 0.4183 - acc: 0.7931 - val_loss: 0.7373 - val_acc: 0.6724
Epoch 11/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3998 - acc: 0.8621 - val_loss: 0.7629 - val_acc: 0.6466
Epoch 12/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3867 - acc: 0.8563 - val_loss: 0.7926 - val_acc: 0.6724
Epoch 13/35

348/348 [==============================] - 20s 57ms/step - loss: 0.3659 - acc: 0.8851 - val_loss: 0.8390 - val_acc: 0.5862
Epoch 14/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3530 - acc: 0.8448 - val_loss: 0.8752 - val_acc: 0.6121
Epoch 15/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3382 - acc: 0.8506 - val_loss: 0.8991 - val_acc: 0.6552
Epoch 16/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3228 - acc: 0.8966 - val_loss: 0.9376 - val_acc: 0.6638
Epoch 17/35

348/348 [==============================] - 20s 58ms/step - loss: 0.3128 - acc: 0.8966 - val_loss: 0.9879 - val_acc: 0.6724
Epoch 18/35

348/348 [==============================] - 20s 57ms/step - loss: 0.2966 - acc: 0.9167 - val_loss: 1.0441 - val_acc: 0.6207
Epoch 19/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2883 - acc: 0.8879 - val_loss: 1.0646 - val_acc: 0.6638
Epoch 20/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2735 - acc: 0.9167 - val_loss: 1.0960 - val_acc: 0.6466
Epoch 21/35

348/348 [==============================] - 20s 57ms/step - loss: 0.2657 - acc: 0.9023 - val_loss: 1.1411 - val_acc: 0.6724
Epoch 22/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2524 - acc: 0.9282 - val_loss: 1.1827 - val_acc: 0.6724
Epoch 23/35

348/348 [==============================] - 20s 57ms/step - loss: 0.2440 - acc: 0.9253 - val_loss: 1.1962 - val_acc: 0.6552
Epoch 24/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2340 - acc: 0.9138 - val_loss: 1.2289 - val_acc: 0.6724
Epoch 25/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2230 - acc: 0.9253 - val_loss: 1.2603 - val_acc: 0.6552
Epoch 26/35

348/348 [==============================] - 20s 58ms/step - loss: 0.2166 - acc: 0.9569 - val_loss: 1.2570 - val_acc: 0.6810
Epoch 27/35

348/348 [==============================] - 20s 57ms/step - loss: 0.2051 - acc: 0.9425 - val_loss: 1.2693 - val_acc: 0.6897
Epoch 28/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1967 - acc: 0.9511 - val_loss: 1.3002 - val_acc: 0.6466
Epoch 29/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1910 - acc: 0.9598 - val_loss: 1.2923 - val_acc: 0.6897
Epoch 30/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1815 - acc: 0.9598 - val_loss: 1.3029 - val_acc: 0.6983
Epoch 31/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1718 - acc: 0.9655 - val_loss: 1.3164 - val_acc: 0.6638
Epoch 32/35

348/348 [==============================] - 20s 57ms/step - loss: 0.1662 - acc: 0.9713 - val_loss: 1.3084 - val_acc: 0.6983
Epoch 33/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1618 - acc: 0.9454 - val_loss: 1.3333 - val_acc: 0.6466
Epoch 34/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1544 - acc: 0.9741 - val_loss: 1.3145 - val_acc: 0.7069
Epoch 35/35

348/348 [==============================] - 20s 58ms/step - loss: 0.1452 - acc: 0.9684 - val_loss: 1.3196 - val_acc: 0.6983
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.67972338e-01 3.20276879e-02]
 [1.91982016e-01 8.08018029e-01]
 [1.00000000e+00 9.25067312e-09]
 [9.65254724e-01 3.47452164e-02]
 [9.81493294e-01 1.85067225e-02]
 [3.87198448e-01 6.12801552e-01]
 [4.15237591e-04 9.99584734e-01]
 [1.00000000e+00 4.91788721e-10]
 [9.90131974e-01 9.86797735e-03]
 [9.99999881e-01 7.32804253e-08]
 [9.79137063e-01 2.08629817e-02]
 [5.43876648e-01 4.56123352e-01]
 [3.14033449e-01 6.85966551e-01]
 [5.09821057e-01 4.90178943e-01]
 [2.51653403e-01 7.48346627e-01]
 [9.99978542e-01 2.15047075e-05]
 [1.83479652e-01 8.16520333e-01]
 [5.47715485e-01 4.52284455e-01]
 [9.95442986e-01 4.55698976e-03]
 [3.91852751e-04 9.99608219e-01]
 [4.15237591e-04 9.99584734e-01]
 [1.83479652e-01 8.16520333e-01]
 [9.02778120e-05 9.99909759e-01]
 [9.81493294e-01 1.85067225e-02]
 [5.09821057e-01 4.90178943e-01]
 [4.70306635e-01 5.29693365e-01]
 [5.08420229e-01 4.91579801e-01]
 [9.97535348e-01 2.46469933e-03]
 [5.80374181e-01 4.19625789e-01]
 [2.51653403e-01 7.48346627e-01]
 [5.92461407e-01 4.07538563e-01]
 [4.22318161e-01 5.77681839e-01]
 [2.76604533e-01 7.23395467e-01]
 [4.21399087e-01 5.78600883e-01]
 [1.00000000e+00 1.02455294e-10]
 [1.32435630e-03 9.98675644e-01]
 [1.00000000e+00 7.12262638e-10]
 [5.43876648e-01 4.56123352e-01]
 [3.67116600e-01 6.32883430e-01]
 [5.92461407e-01 4.07538563e-01]
 [8.37296665e-01 1.62703380e-01]
 [1.41304120e-01 8.58695924e-01]
 [2.87168761e-10 1.00000000e+00]
 [1.00000000e+00 9.25067312e-09]
 [2.95228511e-01 7.04771519e-01]
 [9.52890515e-01 4.71094586e-02]
 [9.37887013e-01 6.21129982e-02]
 [6.11401260e-01 3.88598740e-01]
 [1.44987717e-01 8.55012298e-01]
 [1.00000000e+00 7.12262638e-10]
 [1.65666395e-03 9.98343349e-01]
 [1.00000000e+00 7.12262638e-10]
 [9.99978542e-01 2.15047075e-05]
 [9.79658365e-01 2.03416459e-02]
 [5.92263222e-01 4.07736838e-01]
 [9.89747047e-01 1.02529218e-02]
 [3.49539250e-01 6.50460720e-01]
 [5.92461407e-01 4.07538563e-01]
 [4.20952350e-01 5.79047680e-01]
 [9.81493294e-01 1.85067225e-02]
 [1.00000000e+00 4.91788721e-10]
 [4.70306635e-01 5.29693365e-01]
 [5.92461407e-01 4.07538563e-01]
 [6.28364503e-01 3.71635497e-01]
 [4.15237591e-04 9.99584734e-01]
 [1.31426649e-02 9.86857355e-01]
 [6.28364503e-01 3.71635497e-01]
 [1.44987717e-01 8.55012298e-01]
 [1.48755029e-01 8.51244986e-01]
 [5.71171761e-01 4.28828269e-01]
 [6.29024726e-08 9.99999881e-01]
 [3.15624714e-01 6.84375286e-01]
 [1.44987717e-01 8.55012298e-01]
 [2.69349481e-14 1.00000000e+00]
 [1.41304120e-01 8.58695924e-01]
 [4.71780479e-01 5.28219461e-01]
 [4.42485362e-01 5.57514668e-01]
 [1.00000000e+00 4.91788721e-10]
 [5.43876648e-01 4.56123352e-01]
 [5.43876648e-01 4.56123352e-01]
 [9.99978542e-01 2.15047075e-05]
 [7.64413357e-01 2.35586658e-01]
 [3.59644562e-01 6.40355349e-01]
 [6.11401260e-01 3.88598740e-01]
 [2.27364942e-01 7.72634983e-01]
 [9.99999881e-01 7.32804253e-08]
 [1.00000000e+00 2.38498471e-10]
 [4.22318161e-01 5.77681839e-01]
 [9.99999762e-01 2.46203342e-07]
 [3.33945781e-01 6.66054249e-01]
 [2.76604533e-01 7.23395467e-01]
 [3.17993671e-01 6.82006359e-01]
 [8.17736208e-01 1.82263747e-01]
 [9.99997735e-01 2.24253336e-06]
 [9.98397410e-01 1.60260848e-03]
 [9.65254724e-01 3.47452164e-02]
 [1.00000000e+00 4.91788721e-10]
 [4.22622055e-01 5.77377975e-01]
 [2.76604533e-01 7.23395467e-01]
 [4.43945259e-01 5.56054771e-01]
 [8.69839072e-01 1.30160958e-01]
 [2.87168761e-10 1.00000000e+00]
 [1.00000000e+00 9.25067312e-09]
 [5.67888439e-01 4.32111591e-01]
 [5.69895983e-01 4.30103987e-01]
 [9.99999881e-01 7.32804253e-08]
 [6.84875390e-03 9.93151188e-01]
 [5.06057620e-01 4.93942469e-01]
 [1.00000000e+00 6.00541500e-19]
 [5.92263222e-01 4.07736838e-01]
 [5.92461407e-01 4.07538563e-01]
 [8.45389903e-01 1.54610112e-01]
 [5.92461407e-01 4.07538563e-01]
 [1.00000000e+00 6.00541500e-19]
 [2.73721844e-01 7.26278126e-01]
 [3.99748415e-01 6.00251615e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0
 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1
 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0
 0 0 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 19ms/step
Test loss: 2.0872791388939165
Test accuracy: 0.6810344807032881
[[44 14]
 [23 35]]

Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:03:50.732041: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:03:50.736760: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:03:50.736877: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bd423af430 executing computations on platform Host. Devices:
2019-11-07 09:03:50.736898: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.5]
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 14s - loss: 0.6930 - acc: 0.4885
348/348 [==============================] - 29s 84ms/step - loss: 0.6956 - acc: 0.4626 - val_loss: 0.6851 - val_acc: 0.5259
Epoch 2/25

174/348 [==============>...............] - ETA: 12s - loss: 0.6640 - acc: 0.6322
348/348 [==============================] - 26s 75ms/step - loss: 0.6955 - acc: 0.5345 - val_loss: 0.6809 - val_acc: 0.5086
Epoch 3/25

174/348 [==============>...............] - ETA: 11s - loss: 0.6728 - acc: 0.5057
348/348 [==============================] - 25s 71ms/step - loss: 0.6705 - acc: 0.5115 - val_loss: 0.6783 - val_acc: 0.5517
Epoch 4/25

174/348 [==============>...............] - ETA: 11s - loss: 0.6633 - acc: 0.5287
348/348 [==============================] - 24s 69ms/step - loss: 0.6515 - acc: 0.6322 - val_loss: 0.6600 - val_acc: 0.5690
Epoch 5/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6411 - acc: 0.6782
348/348 [==============================] - 24s 68ms/step - loss: 0.6233 - acc: 0.6408 - val_loss: 0.6621 - val_acc: 0.5862
Epoch 6/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6225 - acc: 0.6092
348/348 [==============================] - 24s 68ms/step - loss: 0.6024 - acc: 0.6178 - val_loss: 0.7064 - val_acc: 0.5000
Epoch 7/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6175 - acc: 0.5862
348/348 [==============================] - 24s 68ms/step - loss: 0.5847 - acc: 0.6264 - val_loss: 0.7390 - val_acc: 0.5862
Epoch 8/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6415 - acc: 0.5977
348/348 [==============================] - 23s 67ms/step - loss: 0.5950 - acc: 0.6466 - val_loss: 0.8132 - val_acc: 0.5000
Epoch 9/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6683 - acc: 0.5517
348/348 [==============================] - 24s 68ms/step - loss: 0.5937 - acc: 0.6236 - val_loss: 0.7576 - val_acc: 0.5862
Epoch 10/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5608 - acc: 0.6609
348/348 [==============================] - 24s 68ms/step - loss: 0.5569 - acc: 0.6523 - val_loss: 0.7962 - val_acc: 0.5172
Epoch 11/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5720 - acc: 0.6264
348/348 [==============================] - 23s 67ms/step - loss: 0.5421 - acc: 0.6638 - val_loss: 0.7515 - val_acc: 0.5948
Epoch 12/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5158 - acc: 0.6782
348/348 [==============================] - 24s 68ms/step - loss: 0.4994 - acc: 0.7213 - val_loss: 0.7655 - val_acc: 0.6121
Epoch 13/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4704 - acc: 0.7989
348/348 [==============================] - 23s 67ms/step - loss: 0.4721 - acc: 0.7672 - val_loss: 0.8034 - val_acc: 0.5000
Epoch 14/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4435 - acc: 0.7414
348/348 [==============================] - 23s 67ms/step - loss: 0.4638 - acc: 0.7385 - val_loss: 0.8310 - val_acc: 0.5690
Epoch 15/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4309 - acc: 0.7701
348/348 [==============================] - 24s 68ms/step - loss: 0.4552 - acc: 0.7701 - val_loss: 0.8567 - val_acc: 0.5776
Epoch 16/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4394 - acc: 0.7356
348/348 [==============================] - 23s 67ms/step - loss: 0.4342 - acc: 0.7385 - val_loss: 0.8856 - val_acc: 0.5776
Epoch 17/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4215 - acc: 0.7644
348/348 [==============================] - 24s 68ms/step - loss: 0.4207 - acc: 0.7701 - val_loss: 0.9260 - val_acc: 0.5862
Epoch 18/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4157 - acc: 0.7586
348/348 [==============================] - 23s 67ms/step - loss: 0.4117 - acc: 0.7672 - val_loss: 0.9630 - val_acc: 0.6121
Epoch 19/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4130 - acc: 0.7644
348/348 [==============================] - 24s 68ms/step - loss: 0.3964 - acc: 0.7931 - val_loss: 1.0057 - val_acc: 0.6034
Epoch 20/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3954 - acc: 0.8046
348/348 [==============================] - 24s 68ms/step - loss: 0.3862 - acc: 0.7730 - val_loss: 1.0525 - val_acc: 0.5776
Epoch 21/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3710 - acc: 0.8218
348/348 [==============================] - 23s 67ms/step - loss: 0.3763 - acc: 0.7989 - val_loss: 1.1100 - val_acc: 0.5690
Epoch 22/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4157 - acc: 0.7529
348/348 [==============================] - 24s 68ms/step - loss: 0.3799 - acc: 0.7787 - val_loss: 1.1302 - val_acc: 0.6121
Epoch 23/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3619 - acc: 0.8276
348/348 [==============================] - 23s 67ms/step - loss: 0.3465 - acc: 0.8276 - val_loss: 1.1900 - val_acc: 0.5948
Epoch 24/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3542 - acc: 0.8161
348/348 [==============================] - 23s 67ms/step - loss: 0.3420 - acc: 0.8075 - val_loss: 1.3183 - val_acc: 0.6207
Epoch 25/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4447 - acc: 0.7586
348/348 [==============================] - 23s 67ms/step - loss: 0.3845 - acc: 0.7845 - val_loss: 1.3278 - val_acc: 0.5690
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.64633822e-01 3.53661776e-02]
 [5.17364919e-01 4.82634991e-01]
 [1.00000000e+00 1.89886808e-08]
 [9.80429530e-01 1.95705164e-02]
 [9.62671399e-01 3.73286009e-02]
 [6.01931036e-01 3.98068935e-01]
 [8.11696111e-04 9.99188244e-01]
 [1.00000000e+00 7.34914993e-12]
 [9.27306652e-01 7.26933852e-02]
 [9.99999762e-01 1.93172795e-07]
 [8.30128372e-01 1.69871613e-01]
 [5.54508626e-01 4.45491314e-01]
 [5.69101393e-01 4.30898637e-01]
 [6.16786480e-01 3.83213550e-01]
 [5.11571705e-01 4.88428235e-01]
 [9.99998212e-01 1.79974927e-06]
 [6.02312088e-01 3.97687823e-01]
 [6.74234986e-01 3.25765014e-01]
 [9.91390586e-01 8.60944949e-03]
 [3.17867845e-03 9.96821284e-01]
 [8.11696111e-04 9.99188244e-01]
 [6.02312088e-01 3.97687823e-01]
 [1.45658562e-02 9.85434175e-01]
 [9.62671399e-01 3.73286009e-02]
 [6.16786480e-01 3.83213550e-01]
 [5.78259349e-01 4.21740621e-01]
 [5.74902356e-01 4.25097704e-01]
 [9.92063701e-01 7.93632679e-03]
 [6.87272608e-01 3.12727392e-01]
 [5.11571705e-01 4.88428235e-01]
 [6.82819605e-01 3.17180455e-01]
 [6.21246278e-01 3.78753662e-01]
 [5.92353702e-01 4.07646298e-01]
 [6.54965281e-01 3.45034689e-01]
 [1.00000000e+00 2.04585482e-09]
 [2.12976076e-02 9.78702426e-01]
 [9.99176085e-01 8.23949289e-04]
 [5.54508626e-01 4.45491314e-01]
 [6.64024770e-01 3.35975200e-01]
 [6.82819605e-01 3.17180455e-01]
 [7.95063853e-01 2.04936177e-01]
 [5.24437845e-01 4.75562125e-01]
 [1.42126652e-07 9.99999881e-01]
 [1.00000000e+00 1.89886808e-08]
 [4.43580627e-01 5.56419432e-01]
 [6.34324968e-01 3.65675002e-01]
 [4.49823439e-01 5.50176620e-01]
 [6.88528121e-01 3.11471850e-01]
 [4.63847160e-01 5.36152840e-01]
 [9.99176085e-01 8.23949289e-04]
 [4.62080305e-03 9.95379210e-01]
 [9.99176085e-01 8.23949289e-04]
 [9.99998212e-01 1.79974927e-06]
 [8.61473620e-01 1.38526395e-01]
 [7.79679894e-01 2.20320106e-01]
 [9.95358765e-01 4.64127818e-03]
 [8.80693555e-01 1.19306482e-01]
 [6.82819605e-01 3.17180455e-01]
 [6.18873537e-01 3.81126463e-01]
 [9.62671399e-01 3.73286009e-02]
 [1.00000000e+00 7.34914993e-12]
 [5.78259349e-01 4.21740621e-01]
 [6.82819605e-01 3.17180455e-01]
 [7.71796346e-01 2.28203714e-01]
 [8.11696111e-04 9.99188244e-01]
 [2.97363728e-01 7.02636361e-01]
 [7.71796346e-01 2.28203714e-01]
 [4.63847160e-01 5.36152840e-01]
 [1.91586688e-01 8.08413267e-01]
 [6.65517092e-01 3.34482908e-01]
 [2.30332502e-04 9.99769628e-01]
 [5.88367701e-01 4.11632270e-01]
 [4.63847160e-01 5.36152840e-01]
 [2.60319182e-04 9.99739707e-01]
 [5.24437845e-01 4.75562125e-01]
 [6.79549694e-01 3.20450306e-01]
 [8.94573569e-01 1.05426423e-01]
 [1.00000000e+00 7.34914993e-12]
 [5.54508626e-01 4.45491314e-01]
 [5.54508626e-01 4.45491314e-01]
 [9.99998212e-01 1.79974927e-06]
 [9.45128083e-01 5.48718795e-02]
 [5.71964979e-01 4.28034991e-01]
 [6.88528121e-01 3.11471850e-01]
 [4.45598394e-01 5.54401577e-01]
 [9.99999762e-01 1.93172795e-07]
 [9.87528563e-01 1.24714635e-02]
 [6.21246278e-01 3.78753662e-01]
 [9.78090048e-01 2.19099112e-02]
 [4.49409187e-01 5.50590813e-01]
 [5.92353702e-01 4.07646298e-01]
 [5.91437101e-01 4.08562928e-01]
 [1.33649230e-01 8.66350830e-01]
 [9.95712161e-01 4.28789901e-03]
 [9.98022079e-01 1.97792659e-03]
 [9.80429530e-01 1.95705164e-02]
 [1.00000000e+00 7.34914993e-12]
 [5.91917336e-01 4.08082664e-01]
 [5.92353702e-01 4.07646298e-01]
 [6.49310529e-01 3.50689381e-01]
 [9.40834165e-01 5.91658577e-02]
 [1.42126652e-07 9.99999881e-01]
 [1.00000000e+00 1.89886808e-08]
 [7.72191226e-01 2.27808759e-01]
 [6.07764602e-01 3.92235428e-01]
 [9.99999762e-01 1.93172795e-07]
 [2.64836162e-01 7.35163867e-01]
 [6.54797077e-01 3.45202953e-01]
 [1.00000000e+00 1.26495134e-10]
 [7.79679894e-01 2.20320106e-01]
 [6.82819605e-01 3.17180455e-01]
 [8.66269290e-01 1.33730665e-01]
 [6.82819605e-01 3.17180455e-01]
 [1.00000000e+00 1.26495134e-10]
 [3.47201973e-01 6.52797997e-01]
 [5.82435131e-01 4.17564899e-01]]
[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1
 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0
 0 0 0 1 0]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 17ms/step
Test loss: 1.5331900695274616
Test accuracy: 0.5431034482758621
[[49  9]
 [44 14]]

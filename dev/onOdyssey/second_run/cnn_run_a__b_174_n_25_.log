Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:03:46.322235: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:03:46.326281: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:03:46.326410: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d2c2b855e0 executing computations on platform Host. Devices:
2019-11-07 09:03:46.326430: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 15s - loss: 0.6930 - acc: 0.5115
348/348 [==============================] - 28s 81ms/step - loss: 0.6886 - acc: 0.5316 - val_loss: 0.6640 - val_acc: 0.5431
Epoch 2/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6378 - acc: 0.6667
348/348 [==============================] - 22s 64ms/step - loss: 0.6182 - acc: 0.7011 - val_loss: 0.6427 - val_acc: 0.5776
Epoch 3/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5581 - acc: 0.7529
348/348 [==============================] - 22s 64ms/step - loss: 0.5491 - acc: 0.7213 - val_loss: 0.6686 - val_acc: 0.6034
Epoch 4/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4935 - acc: 0.7989
348/348 [==============================] - 22s 64ms/step - loss: 0.4923 - acc: 0.7644 - val_loss: 0.7062 - val_acc: 0.6034
Epoch 5/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4541 - acc: 0.8161
348/348 [==============================] - 22s 64ms/step - loss: 0.4394 - acc: 0.8103 - val_loss: 0.7619 - val_acc: 0.5690
Epoch 6/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4092 - acc: 0.7471
348/348 [==============================] - 22s 63ms/step - loss: 0.4008 - acc: 0.8103 - val_loss: 0.7890 - val_acc: 0.6466
Epoch 7/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3771 - acc: 0.8506
348/348 [==============================] - 22s 64ms/step - loss: 0.3671 - acc: 0.8563 - val_loss: 0.8613 - val_acc: 0.6638
Epoch 8/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3608 - acc: 0.8506
348/348 [==============================] - 22s 63ms/step - loss: 0.3365 - acc: 0.8649 - val_loss: 0.9583 - val_acc: 0.5948
Epoch 9/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3180 - acc: 0.8448
348/348 [==============================] - 22s 63ms/step - loss: 0.3115 - acc: 0.8621 - val_loss: 1.0211 - val_acc: 0.6466
Epoch 10/25

174/348 [==============>...............] - ETA: 9s - loss: 0.2891 - acc: 0.9023
348/348 [==============================] - 22s 63ms/step - loss: 0.2852 - acc: 0.8994 - val_loss: 1.1075 - val_acc: 0.6466
Epoch 11/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2738 - acc: 0.9080
348/348 [==============================] - 22s 64ms/step - loss: 0.2641 - acc: 0.9167 - val_loss: 1.1615 - val_acc: 0.6379
Epoch 12/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2374 - acc: 0.9080
348/348 [==============================] - 22s 64ms/step - loss: 0.2423 - acc: 0.9195 - val_loss: 1.2191 - val_acc: 0.6379
Epoch 13/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2262 - acc: 0.9080
348/348 [==============================] - 22s 64ms/step - loss: 0.2229 - acc: 0.9339 - val_loss: 1.2785 - val_acc: 0.6638
Epoch 14/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1951 - acc: 0.9598
348/348 [==============================] - 22s 64ms/step - loss: 0.2046 - acc: 0.9511 - val_loss: 1.2981 - val_acc: 0.6466
Epoch 15/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2016 - acc: 0.9195
348/348 [==============================] - 22s 64ms/step - loss: 0.1902 - acc: 0.9454 - val_loss: 1.3395 - val_acc: 0.6638
Epoch 16/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1801 - acc: 0.9713
348/348 [==============================] - 22s 64ms/step - loss: 0.1743 - acc: 0.9655 - val_loss: 1.3286 - val_acc: 0.6810
Epoch 17/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1781 - acc: 0.9368
348/348 [==============================] - 22s 64ms/step - loss: 0.1614 - acc: 0.9569 - val_loss: 1.3495 - val_acc: 0.6466
Epoch 18/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1500 - acc: 0.9713
348/348 [==============================] - 22s 64ms/step - loss: 0.1441 - acc: 0.9741 - val_loss: 1.3274 - val_acc: 0.6983
Epoch 19/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1204 - acc: 0.9713
348/348 [==============================] - 22s 64ms/step - loss: 0.1353 - acc: 0.9713 - val_loss: 1.3348 - val_acc: 0.7069
Epoch 20/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1182 - acc: 0.9828
348/348 [==============================] - 22s 64ms/step - loss: 0.1195 - acc: 0.9741 - val_loss: 1.3257 - val_acc: 0.7155
Epoch 21/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1152 - acc: 0.9770
348/348 [==============================] - 22s 64ms/step - loss: 0.1134 - acc: 0.9799 - val_loss: 1.4048 - val_acc: 0.6552
Epoch 22/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1286 - acc: 0.9655
348/348 [==============================] - 22s 64ms/step - loss: 0.1099 - acc: 0.9799 - val_loss: 1.3221 - val_acc: 0.7759
Epoch 23/25

174/348 [==============>...............] - ETA: 9s - loss: 0.1040 - acc: 0.9655
348/348 [==============================] - 22s 64ms/step - loss: 0.0971 - acc: 0.9655 - val_loss: 1.4613 - val_acc: 0.6379
Epoch 24/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1148 - acc: 0.9483
348/348 [==============================] - 22s 64ms/step - loss: 0.0985 - acc: 0.9713 - val_loss: 1.3422 - val_acc: 0.7759
Epoch 25/25

174/348 [==============>...............] - ETA: 9s - loss: 0.0715 - acc: 0.9943
348/348 [==============================] - 22s 64ms/step - loss: 0.0930 - acc: 0.9713 - val_loss: 1.3590 - val_acc: 0.7328
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.7990650e-01 2.0093532e-02]
 [2.5725615e-01 7.4274379e-01]
 [1.0000000e+00 4.7738376e-11]
 [9.7531122e-01 2.4688819e-02]
 [9.8403174e-01 1.5968222e-02]
 [3.4613761e-01 6.5386248e-01]
 [4.4060893e-05 9.9995589e-01]
 [1.0000000e+00 1.2098819e-12]
 [9.9898273e-01 1.0172713e-03]
 [1.0000000e+00 5.4805682e-10]
 [9.9994445e-01 5.5506069e-05]
 [8.5123044e-01 1.4876957e-01]
 [2.7605945e-01 7.2394061e-01]
 [5.0404370e-01 4.9595633e-01]
 [5.0678319e-01 4.9321675e-01]
 [9.9973041e-01 2.6963645e-04]
 [8.0393955e-02 9.1960609e-01]
 [5.0532383e-01 4.9467623e-01]
 [9.9950242e-01 4.9758924e-04]
 [2.5865147e-04 9.9974138e-01]
 [4.4060893e-05 9.9995589e-01]
 [8.0393955e-02 9.1960609e-01]
 [2.6405252e-07 9.9999976e-01]
 [9.8403174e-01 1.5968222e-02]
 [5.0404370e-01 4.9595633e-01]
 [4.9778199e-01 5.0221795e-01]
 [4.1466618e-01 5.8533382e-01]
 [9.9971229e-01 2.8771823e-04]
 [6.4922726e-01 3.5077271e-01]
 [5.0678319e-01 4.9321675e-01]
 [8.9677691e-01 1.0322306e-01]
 [3.6658752e-01 6.3341248e-01]
 [1.4100772e-01 8.5899234e-01]
 [3.8623461e-01 6.1376542e-01]
 [1.0000000e+00 2.1980854e-14]
 [3.3686494e-05 9.9996626e-01]
 [1.0000000e+00 2.4747619e-13]
 [8.5123044e-01 1.4876957e-01]
 [2.5245240e-01 7.4754757e-01]
 [8.9677691e-01 1.0322306e-01]
 [9.3256128e-01 6.7438677e-02]
 [4.2705573e-02 9.5729440e-01]
 [3.0743319e-10 1.0000000e+00]
 [1.0000000e+00 4.7738376e-11]
 [2.0717920e-01 7.9282075e-01]
 [9.9901068e-01 9.8928937e-04]
 [9.8142308e-01 1.8576885e-02]
 [7.1957612e-01 2.8042394e-01]
 [3.4394749e-02 9.6560526e-01]
 [1.0000000e+00 2.4747619e-13]
 [1.4902622e-04 9.9985099e-01]
 [1.0000000e+00 2.4747619e-13]
 [9.9973041e-01 2.6963645e-04]
 [9.9845707e-01 1.5429792e-03]
 [4.5880538e-01 5.4119462e-01]
 [9.7608441e-01 2.3915552e-02]
 [1.3982278e-01 8.6017722e-01]
 [8.9677691e-01 1.0322306e-01]
 [3.5710406e-01 6.4289594e-01]
 [9.8403174e-01 1.5968222e-02]
 [1.0000000e+00 1.2098819e-12]
 [4.9778199e-01 5.0221795e-01]
 [8.9677691e-01 1.0322306e-01]
 [7.6905352e-01 2.3094648e-01]
 [4.4060893e-05 9.9995589e-01]
 [6.5923943e-03 9.9340761e-01]
 [7.6905352e-01 2.3094648e-01]
 [3.4394749e-02 9.6560526e-01]
 [6.4351302e-01 3.5648698e-01]
 [6.7047083e-01 3.2952917e-01]
 [7.8889091e-12 1.0000000e+00]
 [2.6947752e-01 7.3052251e-01]
 [3.4394749e-02 9.6560526e-01]
 [1.4856300e-24 1.0000000e+00]
 [4.2705573e-02 9.5729440e-01]
 [4.5012432e-01 5.4987574e-01]
 [3.1988335e-01 6.8011659e-01]
 [1.0000000e+00 1.2098819e-12]
 [8.5123044e-01 1.4876957e-01]
 [8.5123044e-01 1.4876957e-01]
 [9.9973041e-01 2.6963645e-04]
 [8.9794397e-01 1.0205604e-01]
 [6.0492080e-01 3.9507920e-01]
 [7.1957612e-01 2.8042394e-01]
 [1.3812545e-01 8.6187458e-01]
 [1.0000000e+00 5.4805682e-10]
 [1.0000000e+00 1.4445550e-12]
 [3.6658752e-01 6.3341248e-01]
 [1.0000000e+00 1.3550272e-15]
 [2.6051220e-01 7.3948783e-01]
 [1.4100772e-01 8.5899234e-01]
 [2.3442818e-01 7.6557183e-01]
 [9.9967229e-01 3.2774048e-04]
 [1.0000000e+00 2.7300744e-09]
 [9.9964833e-01 3.5165608e-04]
 [9.7531122e-01 2.4688819e-02]
 [1.0000000e+00 1.2098819e-12]
 [3.8499647e-01 6.1500353e-01]
 [1.4100772e-01 8.5899234e-01]
 [4.7734049e-01 5.2265948e-01]
 [9.1899717e-01 8.1002779e-02]
 [3.0743319e-10 1.0000000e+00]
 [1.0000000e+00 4.7738376e-11]
 [6.3619214e-01 3.6380789e-01]
 [4.7331595e-01 5.2668411e-01]
 [1.0000000e+00 5.4805682e-10]
 [4.2147847e-04 9.9957854e-01]
 [5.2718383e-01 4.7281617e-01]
 [1.0000000e+00 3.8604688e-29]
 [4.5880538e-01 5.4119462e-01]
 [8.9677691e-01 1.0322306e-01]
 [9.1738272e-01 8.2617275e-02]
 [8.9677691e-01 1.0322306e-01]
 [1.0000000e+00 3.8604688e-29]
 [6.0619098e-01 3.9380908e-01]
 [3.8911015e-01 6.1088991e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0
 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1
 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0
 0 0 0 0 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 19ms/step
Test loss: 2.2961250954660875
Test accuracy: 0.6724137931034483
[[44 14]
 [24 34]]

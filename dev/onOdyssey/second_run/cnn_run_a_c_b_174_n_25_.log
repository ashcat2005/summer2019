Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:46.140938: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:46.202077: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:46.202344: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b709766ac0 executing computations on platform Host. Devices:
2019-11-07 09:04:46.202395: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
None
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 11s - loss: 0.6935 - acc: 0.5172
348/348 [==============================] - 22s 64ms/step - loss: 0.6900 - acc: 0.5431 - val_loss: 0.6684 - val_acc: 0.5862
Epoch 2/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6534 - acc: 0.6207
348/348 [==============================] - 20s 58ms/step - loss: 0.6433 - acc: 0.6006 - val_loss: 0.6960 - val_acc: 0.5776
Epoch 3/25

174/348 [==============>...............] - ETA: 9s - loss: 0.6507 - acc: 0.5690
348/348 [==============================] - 20s 58ms/step - loss: 0.6505 - acc: 0.5603 - val_loss: 0.6639 - val_acc: 0.5690
Epoch 4/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5784 - acc: 0.7414
348/348 [==============================] - 20s 58ms/step - loss: 0.6564 - acc: 0.6379 - val_loss: 0.7391 - val_acc: 0.5086
Epoch 5/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5736 - acc: 0.6034
348/348 [==============================] - 20s 58ms/step - loss: 0.6244 - acc: 0.5833 - val_loss: 0.7345 - val_acc: 0.5862
Epoch 6/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5783 - acc: 0.6552
348/348 [==============================] - 20s 58ms/step - loss: 0.6511 - acc: 0.6207 - val_loss: 0.7841 - val_acc: 0.5086
Epoch 7/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5594 - acc: 0.6494
348/348 [==============================] - 20s 58ms/step - loss: 0.5997 - acc: 0.6293 - val_loss: 0.7454 - val_acc: 0.5776
Epoch 8/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5117 - acc: 0.7184
348/348 [==============================] - 20s 58ms/step - loss: 0.5709 - acc: 0.6753 - val_loss: 0.7663 - val_acc: 0.5776
Epoch 9/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5150 - acc: 0.7184
348/348 [==============================] - 20s 58ms/step - loss: 0.5871 - acc: 0.6638 - val_loss: 0.7897 - val_acc: 0.5776
Epoch 10/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4759 - acc: 0.7356
348/348 [==============================] - 20s 58ms/step - loss: 0.5692 - acc: 0.6868 - val_loss: 0.8197 - val_acc: 0.5603
Epoch 11/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4961 - acc: 0.7241
348/348 [==============================] - 20s 58ms/step - loss: 0.5532 - acc: 0.6753 - val_loss: 0.8980 - val_acc: 0.5345
Epoch 12/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5142 - acc: 0.6724
348/348 [==============================] - 20s 58ms/step - loss: 0.5320 - acc: 0.6782 - val_loss: 0.9157 - val_acc: 0.5948
Epoch 13/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4693 - acc: 0.7471
348/348 [==============================] - 20s 58ms/step - loss: 0.4692 - acc: 0.7385 - val_loss: 1.0167 - val_acc: 0.5086
Epoch 14/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5110 - acc: 0.6954
348/348 [==============================] - 20s 58ms/step - loss: 0.4848 - acc: 0.7069 - val_loss: 0.9688 - val_acc: 0.5948
Epoch 15/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4747 - acc: 0.7529
348/348 [==============================] - 20s 58ms/step - loss: 0.4827 - acc: 0.7299 - val_loss: 0.9735 - val_acc: 0.5431
Epoch 16/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4536 - acc: 0.7299
348/348 [==============================] - 20s 58ms/step - loss: 0.4678 - acc: 0.7126 - val_loss: 0.9786 - val_acc: 0.5603
Epoch 17/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3747 - acc: 0.7989
348/348 [==============================] - 20s 58ms/step - loss: 0.4523 - acc: 0.7557 - val_loss: 1.0236 - val_acc: 0.5948
Epoch 18/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4365 - acc: 0.7586
348/348 [==============================] - 20s 58ms/step - loss: 0.4302 - acc: 0.7615 - val_loss: 1.0940 - val_acc: 0.5345
Epoch 19/25

174/348 [==============>...............] - ETA: 9s - loss: 0.4506 - acc: 0.6954
348/348 [==============================] - 20s 58ms/step - loss: 0.4201 - acc: 0.7385 - val_loss: 1.0746 - val_acc: 0.5862
Epoch 20/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3735 - acc: 0.7874
348/348 [==============================] - 20s 58ms/step - loss: 0.4072 - acc: 0.7816 - val_loss: 1.0774 - val_acc: 0.5948
Epoch 21/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3977 - acc: 0.7644
348/348 [==============================] - 20s 58ms/step - loss: 0.3783 - acc: 0.7874 - val_loss: 1.1149 - val_acc: 0.6121
Epoch 22/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3661 - acc: 0.7989
348/348 [==============================] - 20s 58ms/step - loss: 0.3758 - acc: 0.7989 - val_loss: 1.1186 - val_acc: 0.5776
Epoch 23/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3472 - acc: 0.8161
348/348 [==============================] - 20s 58ms/step - loss: 0.3731 - acc: 0.8017 - val_loss: 1.1362 - val_acc: 0.5948
Epoch 24/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3728 - acc: 0.7931
348/348 [==============================] - 20s 58ms/step - loss: 0.3615 - acc: 0.8190 - val_loss: 1.1726 - val_acc: 0.6034
Epoch 25/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3690 - acc: 0.7874
348/348 [==============================] - 20s 58ms/step - loss: 0.3521 - acc: 0.8075 - val_loss: 1.1722 - val_acc: 0.5862
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[8.91804457e-01 1.08195558e-01]
 [3.05473000e-01 6.94526970e-01]
 [9.99976397e-01 2.35727057e-05]
 [9.27494705e-01 7.25053176e-02]
 [8.87502611e-01 1.12497389e-01]
 [3.70879054e-01 6.29120946e-01]
 [9.31115530e-04 9.99068916e-01]
 [1.00000000e+00 1.41389664e-08]
 [7.67404616e-01 2.32595369e-01]
 [9.99998450e-01 1.55190594e-06]
 [3.28572869e-01 6.71427131e-01]
 [3.14010561e-01 6.85989439e-01]
 [3.40640277e-01 6.59359753e-01]
 [3.71743023e-01 6.28256917e-01]
 [2.54278153e-01 7.45721877e-01]
 [9.99817193e-01 1.82810894e-04]
 [3.39489281e-01 6.60510719e-01]
 [4.12848890e-01 5.87151110e-01]
 [9.49316621e-01 5.06833829e-02]
 [2.59007863e-03 9.97409880e-01]
 [9.31115530e-04 9.99068916e-01]
 [3.39489281e-01 6.60510719e-01]
 [3.05913389e-03 9.96940792e-01]
 [8.87502611e-01 1.12497389e-01]
 [3.71743023e-01 6.28256917e-01]
 [3.48242939e-01 6.51757061e-01]
 [3.17518234e-01 6.82481706e-01]
 [9.16624367e-01 8.33756328e-02]
 [4.34715062e-01 5.65284908e-01]
 [2.54278153e-01 7.45721877e-01]
 [3.26518029e-01 6.73482060e-01]
 [3.87665421e-01 6.12334609e-01]
 [3.60106558e-01 6.39893413e-01]
 [4.22656745e-01 5.77343225e-01]
 [9.99999642e-01 3.50881237e-07]
 [1.58402547e-02 9.84159768e-01]
 [9.95438874e-01 4.56120353e-03]
 [3.14010561e-01 6.85989439e-01]
 [4.24142599e-01 5.75857401e-01]
 [3.26518029e-01 6.73482060e-01]
 [5.17482579e-01 4.82517362e-01]
 [3.20194811e-01 6.79805219e-01]
 [1.28573288e-06 9.99998689e-01]
 [9.99976397e-01 2.35727057e-05]
 [1.75258175e-01 8.24741900e-01]
 [5.04625022e-01 4.95374978e-01]
 [3.32705408e-01 6.67294621e-01]
 [4.40649092e-01 5.59350908e-01]
 [2.48055309e-01 7.51944721e-01]
 [9.95438874e-01 4.56120353e-03]
 [3.16844811e-03 9.96831596e-01]
 [9.95438874e-01 4.56120353e-03]
 [9.99817193e-01 1.82810894e-04]
 [5.89026570e-01 4.10973370e-01]
 [5.55366158e-01 4.44633812e-01]
 [9.63592112e-01 3.64078693e-02]
 [6.85574234e-01 3.14425826e-01]
 [3.26518029e-01 6.73482060e-01]
 [3.80067915e-01 6.19932115e-01]
 [8.87502611e-01 1.12497389e-01]
 [1.00000000e+00 1.41389664e-08]
 [3.48242939e-01 6.51757061e-01]
 [3.26518029e-01 6.73482060e-01]
 [5.44917762e-01 4.55082268e-01]
 [9.31115530e-04 9.99068916e-01]
 [1.93902314e-01 8.06097686e-01]
 [5.44917762e-01 4.55082268e-01]
 [2.48055309e-01 7.51944721e-01]
 [7.44546279e-02 9.25545335e-01]
 [3.80170077e-01 6.19829893e-01]
 [7.74037791e-04 9.99225974e-01]
 [3.55076492e-01 6.44923508e-01]
 [2.48055309e-01 7.51944721e-01]
 [7.49735907e-03 9.92502570e-01]
 [3.20194811e-01 6.79805219e-01]
 [4.28378522e-01 5.71621478e-01]
 [7.10019469e-01 2.89980531e-01]
 [1.00000000e+00 1.41389664e-08]
 [3.14010561e-01 6.85989439e-01]
 [3.14010561e-01 6.85989439e-01]
 [9.99817193e-01 1.82810894e-04]
 [8.19543362e-01 1.80456653e-01]
 [2.96992421e-01 7.03007519e-01]
 [4.40649092e-01 5.59350908e-01]
 [2.07897678e-01 7.92102337e-01]
 [9.99998450e-01 1.55190594e-06]
 [9.79706347e-01 2.02936847e-02]
 [3.87665421e-01 6.12334609e-01]
 [8.52521420e-01 1.47478521e-01]
 [1.80996761e-01 8.19003224e-01]
 [3.60106558e-01 6.39893413e-01]
 [3.56555164e-01 6.43444836e-01]
 [6.58483654e-02 9.34151649e-01]
 [9.41791356e-01 5.82086891e-02]
 [9.80666339e-01 1.93336681e-02]
 [9.27494705e-01 7.25053176e-02]
 [1.00000000e+00 1.41389664e-08]
 [3.52531075e-01 6.47468925e-01]
 [3.60106558e-01 6.39893413e-01]
 [4.06002492e-01 5.93997538e-01]
 [8.24336350e-01 1.75663725e-01]
 [1.28573288e-06 9.99998689e-01]
 [9.99976397e-01 2.35727057e-05]
 [5.74248552e-01 4.25751448e-01]
 [3.56741846e-01 6.43258154e-01]
 [9.99998450e-01 1.55190594e-06]
 [1.42030299e-01 8.57969701e-01]
 [4.08156723e-01 5.91843307e-01]
 [1.00000000e+00 1.52677454e-10]
 [5.55366158e-01 4.44633812e-01]
 [3.26518029e-01 6.73482060e-01]
 [6.60663784e-01 3.39336216e-01]
 [3.26518029e-01 6.73482060e-01]
 [1.00000000e+00 1.52677454e-10]
 [1.16106115e-01 8.83893847e-01]
 [2.78381467e-01 7.21618593e-01]]
[0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0
 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1
 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1
 0 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 16ms/step
Test loss: 1.2424646328235496
Test accuracy: 0.637931032427426
[[31 27]
 [15 43]]

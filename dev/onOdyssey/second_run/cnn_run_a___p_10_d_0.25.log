Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:05:02.535074: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:05:02.543244: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:05:02.543350: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x555d0b5a6a90 executing computations on platform Host. Devices:
2019-11-07 09:05:02.543371: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.25]
Train on 348 samples, validate on 116 samples
Epoch 1/35

348/348 [==============================] - 15s 44ms/step - loss: 0.6896 - acc: 0.5057 - val_loss: 0.6912 - val_acc: 0.4569
Epoch 2/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6815 - acc: 0.5402 - val_loss: 0.6903 - val_acc: 0.4828
Epoch 3/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6731 - acc: 0.6753 - val_loss: 0.6905 - val_acc: 0.4655
Epoch 4/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6651 - acc: 0.6667 - val_loss: 0.6917 - val_acc: 0.4914
Epoch 5/35

348/348 [==============================] - 13s 36ms/step - loss: 0.6576 - acc: 0.6638 - val_loss: 0.6939 - val_acc: 0.5345
Epoch 6/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6505 - acc: 0.6839 - val_loss: 0.6967 - val_acc: 0.5000
Epoch 7/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6437 - acc: 0.6810 - val_loss: 0.7001 - val_acc: 0.5259
Epoch 8/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6373 - acc: 0.6839 - val_loss: 0.7042 - val_acc: 0.5259
Epoch 9/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6310 - acc: 0.6695 - val_loss: 0.7090 - val_acc: 0.5431
Epoch 10/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6248 - acc: 0.6782 - val_loss: 0.7145 - val_acc: 0.5776
Epoch 11/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6188 - acc: 0.7069 - val_loss: 0.7206 - val_acc: 0.5776
Epoch 12/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6129 - acc: 0.7069 - val_loss: 0.7275 - val_acc: 0.5776
Epoch 13/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6070 - acc: 0.7126 - val_loss: 0.7351 - val_acc: 0.5862
Epoch 14/35

348/348 [==============================] - 13s 37ms/step - loss: 0.6012 - acc: 0.7155 - val_loss: 0.7434 - val_acc: 0.5862
Epoch 15/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5954 - acc: 0.7126 - val_loss: 0.7523 - val_acc: 0.5776
Epoch 16/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5897 - acc: 0.7126 - val_loss: 0.7619 - val_acc: 0.5862
Epoch 17/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5840 - acc: 0.7155 - val_loss: 0.7720 - val_acc: 0.5862
Epoch 18/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5783 - acc: 0.7126 - val_loss: 0.7826 - val_acc: 0.5776
Epoch 19/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5726 - acc: 0.7184 - val_loss: 0.7934 - val_acc: 0.5948
Epoch 20/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5670 - acc: 0.7184 - val_loss: 0.8043 - val_acc: 0.5948
Epoch 21/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5615 - acc: 0.7184 - val_loss: 0.8153 - val_acc: 0.5948
Epoch 22/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5560 - acc: 0.7213 - val_loss: 0.8267 - val_acc: 0.5948
Epoch 23/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5505 - acc: 0.7241 - val_loss: 0.8385 - val_acc: 0.5948
Epoch 24/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5451 - acc: 0.7098 - val_loss: 0.8502 - val_acc: 0.5948
Epoch 25/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5398 - acc: 0.7098 - val_loss: 0.8621 - val_acc: 0.5862
Epoch 26/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5345 - acc: 0.7213 - val_loss: 0.8744 - val_acc: 0.5862
Epoch 27/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5292 - acc: 0.7184 - val_loss: 0.8864 - val_acc: 0.5776
Epoch 28/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5240 - acc: 0.7213 - val_loss: 0.8989 - val_acc: 0.5776
Epoch 29/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5189 - acc: 0.7184 - val_loss: 0.9115 - val_acc: 0.5776
Epoch 30/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5138 - acc: 0.7184 - val_loss: 0.9242 - val_acc: 0.5862
Epoch 31/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5088 - acc: 0.7241 - val_loss: 0.9377 - val_acc: 0.6034
Epoch 32/35

348/348 [==============================] - 13s 37ms/step - loss: 0.5039 - acc: 0.7328 - val_loss: 0.9503 - val_acc: 0.6034
Epoch 33/35

348/348 [==============================] - 13s 37ms/step - loss: 0.4990 - acc: 0.7414 - val_loss: 0.9654 - val_acc: 0.6034
Epoch 34/35

348/348 [==============================] - 13s 37ms/step - loss: 0.4941 - acc: 0.7471 - val_loss: 0.9786 - val_acc: 0.6034
Epoch 35/35

348/348 [==============================] - 13s 37ms/step - loss: 0.4893 - acc: 0.7471 - val_loss: 0.9945 - val_acc: 0.6121
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[6.71935081e-01 3.28064919e-01]
 [4.81377900e-01 5.18622041e-01]
 [8.78141105e-01 1.21858858e-01]
 [8.00931394e-01 1.99068591e-01]
 [4.95705187e-01 5.04294872e-01]
 [4.84721154e-01 5.15278876e-01]
 [4.15643677e-02 9.58435595e-01]
 [9.44699049e-01 5.53010032e-02]
 [5.42853951e-01 4.57146049e-01]
 [9.73784626e-01 2.62154117e-02]
 [5.13039768e-01 4.86960173e-01]
 [4.66223538e-01 5.33776462e-01]
 [4.52207923e-01 5.47792137e-01]
 [4.23781723e-01 5.76218247e-01]
 [3.84207040e-01 6.15792990e-01]
 [9.83861268e-01 1.61386617e-02]
 [2.60297477e-01 7.39702582e-01]
 [5.19770324e-01 4.80229616e-01]
 [5.85408270e-01 4.14591700e-01]
 [2.87804663e-01 7.12195277e-01]
 [4.15643677e-02 9.58435595e-01]
 [2.60297477e-01 7.39702582e-01]
 [2.54774481e-01 7.45225430e-01]
 [4.95705187e-01 5.04294872e-01]
 [4.23781723e-01 5.76218247e-01]
 [3.68131936e-01 6.31868064e-01]
 [4.64983344e-01 5.35016656e-01]
 [3.64339679e-01 6.35660291e-01]
 [4.91277218e-01 5.08722723e-01]
 [3.84207040e-01 6.15792990e-01]
 [4.93231624e-01 5.06768405e-01]
 [4.85143840e-01 5.14856100e-01]
 [4.30605382e-01 5.69394588e-01]
 [5.22423863e-01 4.77576166e-01]
 [9.77185190e-01 2.28148215e-02]
 [4.21619922e-01 5.78380108e-01]
 [1.37362862e-02 9.86263752e-01]
 [4.66223538e-01 5.33776462e-01]
 [4.97837126e-01 5.02162874e-01]
 [4.93231624e-01 5.06768405e-01]
 [2.81129658e-01 7.18870342e-01]
 [4.50463086e-01 5.49536943e-01]
 [5.15818499e-07 9.99999523e-01]
 [8.78141105e-01 1.21858858e-01]
 [4.06161368e-01 5.93838692e-01]
 [7.97720730e-01 2.02279255e-01]
 [3.69383276e-01 6.30616784e-01]
 [4.38587129e-01 5.61412811e-01]
 [3.84890944e-01 6.15109026e-01]
 [1.37362862e-02 9.86263752e-01]
 [2.86901388e-02 9.71309900e-01]
 [1.37362862e-02 9.86263752e-01]
 [9.83861268e-01 1.61386617e-02]
 [6.18179679e-01 3.81820291e-01]
 [5.50362408e-01 4.49637681e-01]
 [9.29408334e-03 9.90705907e-01]
 [5.81154823e-01 4.18845147e-01]
 [4.93231624e-01 5.06768405e-01]
 [4.76207227e-01 5.23792803e-01]
 [4.95705187e-01 5.04294872e-01]
 [9.44699049e-01 5.53010032e-02]
 [3.68131936e-01 6.31868064e-01]
 [4.93231624e-01 5.06768405e-01]
 [6.10970259e-01 3.89029771e-01]
 [4.15643677e-02 9.58435595e-01]
 [7.20031321e-01 2.79968709e-01]
 [6.10970259e-01 3.89029771e-01]
 [3.84890944e-01 6.15109026e-01]
 [4.44732666e-01 5.55267334e-01]
 [5.69077671e-01 4.30922329e-01]
 [8.96685898e-01 1.03314124e-01]
 [4.50623363e-01 5.49376607e-01]
 [3.84890944e-01 6.15109026e-01]
 [1.00000000e+00 1.65026872e-08]
 [4.50463086e-01 5.49536943e-01]
 [4.82809305e-01 5.17190635e-01]
 [6.24147475e-01 3.75852555e-01]
 [9.44699049e-01 5.53010032e-02]
 [4.66223538e-01 5.33776462e-01]
 [4.66223538e-01 5.33776462e-01]
 [9.83861268e-01 1.61386617e-02]
 [5.59448421e-01 4.40551579e-01]
 [3.54645610e-01 6.45354390e-01]
 [4.38587129e-01 5.61412811e-01]
 [4.31247413e-01 5.68752646e-01]
 [9.73784626e-01 2.62154117e-02]
 [9.89740875e-06 9.99990106e-01]
 [4.85143840e-01 5.14856100e-01]
 [1.22107076e-03 9.98778880e-01]
 [4.08154219e-01 5.91845810e-01]
 [4.30605382e-01 5.69394588e-01]
 [4.58968252e-01 5.41031718e-01]
 [8.94844756e-02 9.10515547e-01]
 [8.92524838e-01 1.07475162e-01]
 [8.86047602e-01 1.13952354e-01]
 [8.00931394e-01 1.99068591e-01]
 [9.44699049e-01 5.53010032e-02]
 [4.45481181e-01 5.54518759e-01]
 [4.30605382e-01 5.69394588e-01]
 [4.76164073e-01 5.23835957e-01]
 [9.50006127e-01 4.99938875e-02]
 [5.15818499e-07 9.99999523e-01]
 [8.78141105e-01 1.21858858e-01]
 [5.48591316e-01 4.51408684e-01]
 [4.83559608e-01 5.16440392e-01]
 [9.73784626e-01 2.62154117e-02]
 [4.28444147e-01 5.71555912e-01]
 [4.88769889e-01 5.11230171e-01]
 [9.99916196e-01 8.38485503e-05]
 [5.50362408e-01 4.49637681e-01]
 [4.93231624e-01 5.06768405e-01]
 [5.59924304e-01 4.40075636e-01]
 [4.93231624e-01 5.06768405e-01]
 [9.99916196e-01 8.38485503e-05]
 [4.32372510e-01 5.67627430e-01]
 [4.33274686e-01 5.66725314e-01]]
[0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1
 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0
 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1
 0 1 0 1 1]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 1s 12ms/step
Test loss: 0.8726281379831249
Test accuracy: 0.637931032427426
[[29 29]
 [13 45]]

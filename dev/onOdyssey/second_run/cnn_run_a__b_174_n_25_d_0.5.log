Using TensorFlow backend.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
cnn_aardvark_aug_concat.py:397: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  model = Model(input=ins, output=dense2)
WARNING:tensorflow:From /n/home00/nchou/.conda/envs/envi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-11-07 09:04:55.527125: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:04:55.625795: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2019-11-07 09:04:55.626044: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x565400d22140 executing computations on platform Host. Devices:
2019-11-07 09:04:55.626096: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
[0.5]
Train on 348 samples, validate on 116 samples
Epoch 1/25

174/348 [==============>...............] - ETA: 17s - loss: 0.6939 - acc: 0.4483
348/348 [==============================] - 30s 87ms/step - loss: 0.6910 - acc: 0.5057 - val_loss: 0.6614 - val_acc: 0.6293
Epoch 2/25

174/348 [==============>...............] - ETA: 10s - loss: 0.6413 - acc: 0.8276
348/348 [==============================] - 22s 65ms/step - loss: 0.6309 - acc: 0.7299 - val_loss: 0.6466 - val_acc: 0.6121
Epoch 3/25

174/348 [==============>...............] - ETA: 10s - loss: 0.5741 - acc: 0.8333
348/348 [==============================] - 22s 64ms/step - loss: 0.5683 - acc: 0.7701 - val_loss: 0.6579 - val_acc: 0.5862
Epoch 4/25

174/348 [==============>...............] - ETA: 9s - loss: 0.5343 - acc: 0.7874
348/348 [==============================] - 22s 64ms/step - loss: 0.5128 - acc: 0.7500 - val_loss: 0.6837 - val_acc: 0.6207
Epoch 5/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4588 - acc: 0.8678
348/348 [==============================] - 22s 64ms/step - loss: 0.4663 - acc: 0.8190 - val_loss: 0.7360 - val_acc: 0.5603
Epoch 6/25

174/348 [==============>...............] - ETA: 10s - loss: 0.4439 - acc: 0.7471
348/348 [==============================] - 22s 64ms/step - loss: 0.4267 - acc: 0.7960 - val_loss: 0.7510 - val_acc: 0.6983
Epoch 7/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3653 - acc: 0.8851
348/348 [==============================] - 22s 64ms/step - loss: 0.3878 - acc: 0.8879 - val_loss: 0.7982 - val_acc: 0.6552
Epoch 8/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3821 - acc: 0.8736
348/348 [==============================] - 22s 64ms/step - loss: 0.3590 - acc: 0.8764 - val_loss: 0.8654 - val_acc: 0.6379
Epoch 9/25

174/348 [==============>...............] - ETA: 9s - loss: 0.3123 - acc: 0.9138
348/348 [==============================] - 22s 64ms/step - loss: 0.3314 - acc: 0.9023 - val_loss: 0.9448 - val_acc: 0.6810
Epoch 10/25

174/348 [==============>...............] - ETA: 10s - loss: 0.3327 - acc: 0.8908
348/348 [==============================] - 22s 64ms/step - loss: 0.3088 - acc: 0.8937 - val_loss: 1.0304 - val_acc: 0.6034
Epoch 11/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2991 - acc: 0.8448
348/348 [==============================] - 22s 64ms/step - loss: 0.2884 - acc: 0.8793 - val_loss: 1.0658 - val_acc: 0.6293
Epoch 12/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2643 - acc: 0.9023
348/348 [==============================] - 22s 64ms/step - loss: 0.2647 - acc: 0.9138 - val_loss: 1.1365 - val_acc: 0.6724
Epoch 13/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2692 - acc: 0.9310
348/348 [==============================] - 22s 64ms/step - loss: 0.2558 - acc: 0.9138 - val_loss: 1.1835 - val_acc: 0.6724
Epoch 14/25

174/348 [==============>...............] - ETA: 9s - loss: 0.1923 - acc: 0.9655
348/348 [==============================] - 22s 64ms/step - loss: 0.2486 - acc: 0.9023 - val_loss: 1.2069 - val_acc: 0.6810
Epoch 15/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2250 - acc: 0.9080
348/348 [==============================] - 22s 64ms/step - loss: 0.2412 - acc: 0.8908 - val_loss: 1.2080 - val_acc: 0.6983
Epoch 16/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2010 - acc: 0.9540
348/348 [==============================] - 22s 64ms/step - loss: 0.2130 - acc: 0.9368 - val_loss: 1.2471 - val_acc: 0.6466
Epoch 17/25

174/348 [==============>...............] - ETA: 10s - loss: 0.2188 - acc: 0.9425
348/348 [==============================] - 22s 64ms/step - loss: 0.1992 - acc: 0.9368 - val_loss: 1.2289 - val_acc: 0.6810
Epoch 18/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1914 - acc: 0.9195
348/348 [==============================] - 22s 63ms/step - loss: 0.1908 - acc: 0.9282 - val_loss: 1.2526 - val_acc: 0.6897
Epoch 19/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1759 - acc: 0.9598
348/348 [==============================] - 22s 64ms/step - loss: 0.1785 - acc: 0.9569 - val_loss: 1.2773 - val_acc: 0.6552
Epoch 20/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1680 - acc: 0.9540
348/348 [==============================] - 22s 64ms/step - loss: 0.1586 - acc: 0.9684 - val_loss: 1.2696 - val_acc: 0.7069
Epoch 21/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1582 - acc: 0.9425
348/348 [==============================] - 22s 64ms/step - loss: 0.1546 - acc: 0.9454 - val_loss: 1.2845 - val_acc: 0.6897
Epoch 22/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1407 - acc: 0.9770
348/348 [==============================] - 22s 64ms/step - loss: 0.1460 - acc: 0.9741 - val_loss: 1.3081 - val_acc: 0.6897
Epoch 23/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1369 - acc: 0.9828
348/348 [==============================] - 22s 64ms/step - loss: 0.1294 - acc: 0.9856 - val_loss: 1.2900 - val_acc: 0.7069
Epoch 24/25

174/348 [==============>...............] - ETA: 10s - loss: 0.1355 - acc: 0.9598
348/348 [==============================] - 22s 64ms/step - loss: 0.1269 - acc: 0.9598 - val_loss: 1.3009 - val_acc: 0.6983
Epoch 25/25

174/348 [==============>...............] - ETA: 9s - loss: 0.1035 - acc: 0.9885
348/348 [==============================] - 22s 64ms/step - loss: 0.1162 - acc: 0.9856 - val_loss: 1.3155 - val_acc: 0.6897
Saved trained model at /n/holylfs03/LABS/berger_lab/nchou/saved_models/aardvark_aug11.h5 
[[9.8629445e-01 1.3705531e-02]
 [4.2451292e-01 5.7548708e-01]
 [9.9999988e-01 8.0131265e-08]
 [9.8534888e-01 1.4651108e-02]
 [9.8344731e-01 1.6552703e-02]
 [4.7991160e-01 5.2008843e-01]
 [6.0238427e-04 9.9939764e-01]
 [1.0000000e+00 6.0225651e-11]
 [9.9702436e-01 2.9756550e-03]
 [9.9999988e-01 8.6517382e-08]
 [9.9717963e-01 2.8203439e-03]
 [7.0863783e-01 2.9136223e-01]
 [3.8371775e-01 6.1628222e-01]
 [5.7225770e-01 4.2774227e-01]
 [4.5695904e-01 5.4304099e-01]
 [9.9935216e-01 6.4788928e-04]
 [1.7625672e-01 8.2374328e-01]
 [6.1157596e-01 3.8842404e-01]
 [9.9843544e-01 1.5645019e-03]
 [4.8000613e-04 9.9951994e-01]
 [6.0238427e-04 9.9939764e-01]
 [1.7625672e-01 8.2374328e-01]
 [5.5862754e-05 9.9994409e-01]
 [9.8344731e-01 1.6552703e-02]
 [5.7225770e-01 4.2774227e-01]
 [5.9444994e-01 4.0555009e-01]
 [5.5866873e-01 4.4133124e-01]
 [9.9952865e-01 4.7134957e-04]
 [6.6170073e-01 3.3829924e-01]
 [4.5695904e-01 5.4304099e-01]
 [8.4217483e-01 1.5782516e-01]
 [4.9132234e-01 5.0867766e-01]
 [2.8625834e-01 7.1374160e-01]
 [4.9437219e-01 5.0562775e-01]
 [1.0000000e+00 9.9476295e-12]
 [7.5996382e-04 9.9924004e-01]
 [1.0000000e+00 7.0700759e-12]
 [7.0863783e-01 2.9136223e-01]
 [3.7305772e-01 6.2694228e-01]
 [8.4217483e-01 1.5782516e-01]
 [9.0997219e-01 9.0027727e-02]
 [1.1255685e-01 8.8744318e-01]
 [7.5408249e-09 1.0000000e+00]
 [9.9999988e-01 8.0131265e-08]
 [3.9537588e-01 6.0462409e-01]
 [9.7592574e-01 2.4074275e-02]
 [9.5466620e-01 4.5333765e-02]
 [7.2533363e-01 2.7466637e-01]
 [9.1790155e-02 9.0820992e-01]
 [1.0000000e+00 7.0700759e-12]
 [9.3351840e-04 9.9906653e-01]
 [1.0000000e+00 7.0700759e-12]
 [9.9935216e-01 6.4788928e-04]
 [9.9446493e-01 5.5351178e-03]
 [6.5110427e-01 3.4889573e-01]
 [9.8656297e-01 1.3436961e-02]
 [4.0978283e-01 5.9021717e-01]
 [8.4217483e-01 1.5782516e-01]
 [4.6957639e-01 5.3042370e-01]
 [9.8344731e-01 1.6552703e-02]
 [1.0000000e+00 6.0225651e-11]
 [5.9444994e-01 4.0555009e-01]
 [8.4217483e-01 1.5782516e-01]
 [7.6116139e-01 2.3883861e-01]
 [6.0238427e-04 9.9939764e-01]
 [1.0276830e-02 9.8972321e-01]
 [7.6116139e-01 2.3883861e-01]
 [9.1790155e-02 9.0820992e-01]
 [4.2073220e-01 5.7926774e-01]
 [6.9561064e-01 3.0438939e-01]
 [4.0259348e-08 1.0000000e+00]
 [3.8845292e-01 6.1154711e-01]
 [9.1790155e-02 9.0820992e-01]
 [7.7346559e-16 1.0000000e+00]
 [1.1255685e-01 8.8744318e-01]
 [5.4519284e-01 4.5480716e-01]
 [6.1677909e-01 3.8322094e-01]
 [1.0000000e+00 6.0225651e-11]
 [7.0863783e-01 2.9136223e-01]
 [7.0863783e-01 2.9136223e-01]
 [9.9935216e-01 6.4788928e-04]
 [8.9591038e-01 1.0408955e-01]
 [4.9321994e-01 5.0678009e-01]
 [7.2533363e-01 2.7466637e-01]
 [2.9175189e-01 7.0824808e-01]
 [9.9999988e-01 8.6517382e-08]
 [1.0000000e+00 2.0188820e-10]
 [4.9132234e-01 5.0867766e-01]
 [1.0000000e+00 3.0004813e-10]
 [4.4852808e-01 5.5147189e-01]
 [2.8625834e-01 7.1374160e-01]
 [3.5375515e-01 6.4624494e-01]
 [9.9752218e-01 2.4778035e-03]
 [9.9999750e-01 2.4724056e-06]
 [9.9958199e-01 4.1804041e-04]
 [9.8534888e-01 1.4651108e-02]
 [1.0000000e+00 6.0225651e-11]
 [4.8536894e-01 5.1463103e-01]
 [2.8625834e-01 7.1374160e-01]
 [5.3270167e-01 4.6729836e-01]
 [8.5064101e-01 1.4935906e-01]
 [7.5408249e-09 1.0000000e+00]
 [9.9999988e-01 8.0131265e-08]
 [6.5844005e-01 3.4155998e-01]
 [6.0122687e-01 3.9877319e-01]
 [9.9999988e-01 8.6517382e-08]
 [4.9368539e-03 9.9506313e-01]
 [5.7507765e-01 4.2492232e-01]
 [1.0000000e+00 1.5398130e-20]
 [6.5110427e-01 3.4889573e-01]
 [8.4217483e-01 1.5782516e-01]
 [9.1818470e-01 8.1815250e-02]
 [8.4217483e-01 1.5782516e-01]
 [1.0000000e+00 1.5398130e-20]
 [5.6974757e-01 4.3025246e-01]
 [5.1725954e-01 4.8274052e-01]]
[0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0
 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1
 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0
 0 0 0 0 0]

 32/116 [=======>......................] - ETA: 1s
 64/116 [===============>..............] - ETA: 0s
 96/116 [=======================>......] - ETA: 0s
116/116 [==============================] - 2s 19ms/step
Test loss: 2.160631229137552
Test accuracy: 0.6896551683031279
[[48 10]
 [26 32]]
